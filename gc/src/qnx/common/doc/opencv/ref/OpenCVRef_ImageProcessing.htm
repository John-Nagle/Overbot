<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html><head>
<link rel="STYLESHEET" href="opencvref.css" charset="ISO-8859-1" type="text/css">
<title>OpenCV: Image Processing and Analysis Reference</title>
</head><body>

<h1>Image Processing and Analysis Reference</h1>

<hr><p><ul>
<li><a href="#ch2_drawing">Drawing Functions</a>
<ul>
<li><a href="#decl_cvLine">Line</a>
<li><a href="#decl_cvLineAA">LineAA</a>
<li><a href="#decl_cvRectangle">Rectangle</a>
<li><a href="#decl_cvCircle">Circle</a>
<li><a href="#decl_cvEllipse">Ellipse</a>
<li><a href="#decl_cvEllipseAA">EllipseAA</a>
<li><a href="#decl_cvFillPoly">FillPoly</a>
<li><a href="#decl_cvFillConvexPoly">FillConvexPoly</a>
<li><a href="#decl_cvPolyLine">PolyLine</a>
<li><a href="#decl_cvPolyLineAA">PolyLineAA</a>
<li><a href="#decl_cvInitFont">InitFont</a>
<li><a href="#decl_cvPutText">PutText</a>
<li><a href="#decl_cvGetTextSize">GetTextSize</a>
</ul>
<li><a href="#ch2_features">Gradients, Edges and Corners</a>
<ul>
<li><a href="#decl_cvSobel">Sobel</a>
<li><a href="#decl_cvLaplace">Laplace</a>
<li><a href="#decl_cvCanny">Canny</a>
<li><a href="#decl_cvPreCornerDetect">PreCornerDetect</a>
<li><a href="#decl_cvCornerEigenValsAndVecs">CornerEigenValsAndVecs</a>
<li><a href="#decl_cvCornerMinEigenVal">CornerMinEigenVal</a>
<li><a href="#decl_cvFindCornerSubPix">FindCornerSubPix</a>
<li><a href="#decl_cvGoodFeaturesToTrack">GoodFeaturesToTrack</a>
</ul>
<li><a href="#ch2_geometry">Sampling, Interpolation and Geometrical Transforms</a>
<ul>
<li><a href="#decl_cvInitLineIterator">InitLineIterator</a>
<li><a href="#decl_cvSampleLine">SampleLine</a>
<li><a href="#decl_cvGetRectSubPix">GetRectSubPix</a>
<li><a href="#decl_cvGetQuadrangeSubPix">GetQuadrangeSubPix</a>
<li><a href="#decl_cvResize">Resize</a>
</ul>
<li><a href="#ch2_morphology">Morphological Operations</a>
<ul>
<li><a href="#decl_cvCreateStructuringElementEx">CreateStructuringElementEx</a>
<li><a href="#decl_cvReleaseStructuringElement">ReleaseStructuringElement</a>
<li><a href="#decl_cvErode">Erode</a>
<li><a href="#decl_cvDilate">Dilate</a>
<li><a href="#decl_cvMorphologyEx">MorphologyEx</a>
</ul>
<li><a href="#ch2_filters">Filters and Color Conversion</a>
<ul>
<li><a href="#decl_cvSmooth">Smooth</a>
<li><a href="#decl_cvIntegral">Integral</a>
<li><a href="#decl_cvCvtColor">CvtColor</a>
<li><a href="#decl_cvThreshold">Threshold</a>
<li><a href="#decl_cvAdaptiveThreshold">AdaptiveThreshold</a>
<li><a href="#decl_cvLUT">LUT</a>
</ul>
<li><a href="#ch2_pyramids">Pyramids and the Applications</a>
<ul>
<li><a href="#decl_cvPyrDown">PyrDown</a>
<li><a href="#decl_cvPyrUp">PyrUp</a>
<li><a href="#decl_cvPyrSegmentation">PyrSegmentation</a>
</ul>
<li><a href="#ch2_ccomp">Connected components</a>
<ul>
<li><a href="#decl_CvConnectedComp">ConnectedComp</a>
<li><a href="#decl_cvFloodFill">FloodFill</a>
<li><a href="#decl_cvFindContours">FindContours</a>
<li><a href="#decl_cvStartFindContours">StartFindContours</a>
<li><a href="#decl_cvFindNextContour">FindNextContour</a>
<li><a href="#decl_cvSubstituteContour">SubstituteContour</a>
<li><a href="#decl_cvEndFindContours">EndFindContours</a>
<li><a href="#decl_cvDrawContours">DrawContours</a>
</ul>
<li><a href="#ch2_moments">Image and contour moments</a>
<ul>
<li><a href="#decl_cvMoments">Moments</a>
<li><a href="#decl_cvGetSpatialMoment">GetSpatialMoment</a>
<li><a href="#decl_cvGetCentralMoment">GetCentralMoment</a>
<li><a href="#decl_cvGetNormalizedCentralMoment">GetNormalizedCentralMoment</a>
<li><a href="#decl_cvGetHuMoments">GetHuMoments</a>
</ul>
<li><a href="#ch2_transforms">Special Image Transforms</a>
<ul>
<li><a href="#decl_cvHoughLines">HoughLines</a>
<li><a href="#decl_cvDistTransform">DistTransform</a>
</ul>
<li><a href="#ch2_histograms">Histogram Functions</a>
<ul>
<li><a href="#decl_CvHistogram ">Histogram </a>
<li><a href="#decl_cvCreateHist">CreateHist</a>
<li><a href="#decl_cvSetHistBinRanges">SetHistBinRanges</a>
<li><a href="#decl_cvReleaseHist">ReleaseHist</a>
<li><a href="#decl_cvClearHist">ClearHist</a>
<li><a href="#decl_cvMakeHistHeaderForArray">MakeHistHeaderForArray</a>
<li><a href="#decl_cvQueryHistValue_1D">QueryHistValue_1D</a>
<li><a href="#decl_cvGetHistValue_1D">GetHistValue_1D</a>
<li><a href="#decl_cvGetMinMaxHistValue">GetMinMaxHistValue</a>
<li><a href="#decl_cvNormalizeHist">NormalizeHist</a>
<li><a href="#decl_cvThreshHist">ThreshHist</a>
<li><a href="#decl_cvCompareHist">CompareHist</a>
<li><a href="#decl_cvCopyHist">CopyHist</a>
<li><a href="#decl_cvCalcHist">CalcHist</a>
<li><a href="#decl_cvCalcBackProject">CalcBackProject</a>
<li><a href="#decl_cvCalcBackProjectPatch">CalcBackProjectPatch</a>
<li><a href="#decl_cvCalcProbDensity">CalcProbDensity</a>
<li><a href="#decl_cvCalcEMD2">CalcEMD2</a>
</ul>
<li><a href="#ch2_utilities">Utility Functions</a>
<ul>
<li><a href="#decl_cvMatchTemplate">MatchTemplate</a>
</ul></ul></p>


<p>
Note:<br>
The chapter describes functions for image processing and analysis.
Most of the functions work with 2d arrays of pixels. We refer the arrays
as "images" however they do not neccesserily have to be IplImage's, they may
be CvMat's or CvMatND's as well.
</p>


<hr><h2><a name="ch2_drawing">Drawing Functions</a></h2>

<p>
Drawing functions work with arbitrary 8-bit images or single-channel images
with larger depth: 16s, 32s, 32f, 64f
All the functions include parameter color that means rgb value (that may be
constructed with <code>CV_RGB</code> macro) for color images and brightness
for grayscale images.</p><p>
If a drawn figure is partially or completely outside the image, it is clipped.
</p>

<hr><h3><a name="decl_CV_RGB">CV_RGB</a></h3>
<p class="Blurb">Constructs a color value</p>
<pre>
#define CV_RGB( r, g, b )  (int)((uchar)(b) + ((uchar)(g) &lt;&lt; 8) + ((uchar)(r) &lt;&lt; 16))
</pre>


<hr><h3><a name="decl_cvLine">Line</a></h3>
<p class="Blurb">Draws simple or thick line segment</p>
<pre>
void cvLine( CvArr* img, CvPoint pt1, CvPoint pt2, double color, int thickness=1, int connectivity=8 );
</pre><p><dl>
<dt>img<dd>The image.
<dt>pt1<dd>First point of the line segment.
<dt>pt2<dd>Second point of the line segment.
<dt>color<dd>Line color (RGB) or brightness (grayscale image).
<dt>thickness<dd>Line thickness.
<dt>connectivity<dd>Line connectivity, 8 (by default) or 4. It is possible to pass 0 instead of 8.
</dl><p>
The function <a href="#decl_cvLine">cvLine</a> draws the line segment between <code>pt1</code> and <code>pt2</code> points in the
image. The line is clipped by the image or ROI rectangle. The 8-connected or 4-connected Bresenham
algorithm is used for simple line segments. Thick lines are drawn with rounding
endings. To specify the line color, the user may use the macro <code>CV_RGB( r, g, b )</code>.</p>


<hr><h3><a name="decl_cvLineAA">LineAA</a></h3>
<p class="Blurb">Draws antialiased line segment</p>
<pre>
void cvLineAA( CvArr* img, CvPoint pt1, CvPoint pt2, double color, int scale=0 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pt1<dd>First point of the line segment.
<dt>pt2<dd>Second point of the line segment.
<dt>color<dd>Line color (RGB) or brightness (grayscale image).
<dt>scale<dd>Number of fractional bits in the end point coordinates.
</dl><p>
The function <a href="#decl_cvLineAA">cvLineAA</a> draws the 8-connected line segment between <code>pt1</code> and <code>pt2</code>
points in the image. The line is clipped by the image or ROI rectangle. The algorithm
includes some sort of Gaussian filtering to get a smooth picture. To specify the
line color, the user may use the macro <code>CV_RGB( r, g, b )</code>.</p>


<hr><h3><a name="decl_cvRectangle">Rectangle</a></h3>
<p class="Blurb">Draws simple, thick or filled rectangle</p>
<pre>
void cvRectangle( CvArr* img, CvPoint pt1, CvPoint pt2, double color, int thickness=1 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pt1<dd>One of the rectangle vertices.
<dt>pt2<dd>Opposite rectangle vertex.
<dt>color<dd>Line color (RGB) or brightness (grayscale image).
<dt>thickness<dd>Thickness of lines that make up the rectangle. Negative values, e.g. CV_FILLED,
make the function to draw a filled rectangle.
</dl><p>
The function <a href="#decl_cvRectangle">cvRectangle</a> draws a rectangle with
two opposite corners <code>pt1</code> and <code>pt2</code>.</p>


<hr><h3><a name="decl_cvCircle">Circle</a></h3>
<p class="Blurb">Draws simple, thick or filled circle</p>
<pre>
void cvCircle( CvArr* img, CvPoint center, int radius, double color, int thickness=1 );
</pre><p><dl>
<dt>img<dd>Image where the line is drawn.
<dt>center<dd>Center of the circle.
<dt>radius<dd>Radius of the circle.
<dt>color<dd>Circle color (RGB) or brightness (grayscale image).
<dt>thickness<dd>Thickness of the circle outline if positive, otherwise indicates that
a filled circle has to be drawn.
</dl><p>
The function <a href="#decl_cvCircle">cvCircle</a> draws a simple or filled circle with given center and
radius. The circle is clipped by ROI rectangle. The Bresenham algorithm is used
both for simple and filled circles. To specify the circle color, the user may
use the macro <code>CV_RGB ( r, g, b )</code>.</p>


<hr><h3><a name="decl_cvEllipse">Ellipse</a></h3>
<p class="Blurb">Draws simple or thick elliptic arc or fills ellipse sector</p>
<pre>
void cvEllipse( CvArr* img, CvPoint center, CvSize axes, double angle,
                double startAngle, double endAngle, double color, int thickness=1 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>center<dd>Center of the ellipse.
<dt>axes<dd>Length of the ellipse axes.
<dt>angle<dd>Rotation angle.
<dt>startAngle<dd>Starting angle of the elliptic arc.
<dt>endAngle<dd>Ending angle of the elliptic arc.
<dt>color<dd>Ellipse color (RGB) or brightness (grayscale image).
<dt>thickness<dd>Thickness of the ellipse arc.
</dl><p>
The function <a href="#decl_cvEllipse">cvEllipse</a> draws a simple or thick elliptic arc or fills an ellipse
sector. The arc is clipped by ROI rectangle. The generalized Bresenham algorithm
for conic section is used for simple elliptic arcs here, and piecewise-linear
approximation is used for antialiased arcs and thick arcs. All the angles are
given in degrees. The picture below explains the meaning of the parameters.</p>
<p>
<font color=blue>Parameters of Elliptic Arc</font>
</p>
<p>
<img align="center" src="pics/ellipse.png" >
</p>


<hr><h3><a name="decl_cvEllipseAA">EllipseAA</a></h3>
<p class="Blurb">Draws antialiased elliptic arc</p>
<pre>
void cvEllipseAA( CvArr* img, CvPoint center, CvSize axes, double angle,
                  double startAngle, double endAngle, double color, int scale=0 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>center<dd>Center of the ellipse.
<dt>axes<dd>Length of the ellipse axes.
<dt>angle<dd>Rotation angle.
<dt>startAngle<dd>Starting angle of the elliptic arc.
<dt>endAngle<dd>Ending angle of the elliptic arc.
<dt>color<dd>Ellipse color (RGB) or brightness (grayscale image).
<dt>scale<dd>Specifies the number of fractional bits in the center coordinates and axes
sizes.
</dl><p>
The function <a href="#decl_cvEllipseAA">cvEllipseAA</a> draws an antialiased elliptic arc. The arc is clipped by
ROI rectangle. The generalized Bresenham algorithm for conic section is used for
simple elliptic arcs here, and piecewise-linear approximation is used for
antialiased arcs and thick arcs. All the angles are in degrees.


<hr><h3><a name="decl_cvFillPoly">FillPoly</a></h3>
<p class="Blurb">Fills polygons interior</p>
<pre>
void cvFillPoly( CvArr* img, CvPoint** pts, int* npts, int contours, double color );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pts<dd>Array of pointers to polygons.
<dt>npts<dd>Array of polygon vertex counters.
<dt>contours<dd>Number of contours that bind the filled region.
<dt>color<dd>Polygon color (RGB) or brightness (grayscale image).
</dl><p>
The function <a href="#decl_cvFillPoly">cvFillPoly</a> fills an area bounded by several polygonal contours. The
function fills complex areas, for example, areas with holes, contour
self-intersection, etc.</p>


<hr><h3><a name="decl_cvFillConvexPoly">FillConvexPoly</a></h3>
<p class="Blurb">Fills convex polygon</p>
<pre>
void cvFillConvexPoly( CvArr* img, CvPoint* pts, int npts, double color );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pts<dd>Array of pointers to a single polygon.
<dt>npts<dd>Polygon vertex counter.
<dt>color<dd>Polygon color (RGB) or brightness (grayscale image).
</dl><p>
The function <a href="#decl_cvFillConvexPoly">cvFillConvexPoly</a> fills convex polygon interior. This function is much
faster than the function <a href="#decl_cvFillPoly">cvFillPoly</a> and fills not only the convex polygon but
any monotonic polygon, that is, a polygon whose contour intersects every
horizontal line (scan line) twice at the most.</p>


<hr><h3><a name="decl_cvPolyLine">PolyLine</a></h3>
<p class="Blurb">Draws simple or thick polygons</p>
<pre>
void cvPolyLine( CvArr* img, CvPoint** pts, int* npts, int contours, int isClosed,
                 double color, int thickness=1, int connectivity=8 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pts<dd>Array of pointers to polylines.
<dt>npts<dd>Array of polyline vertex counters.
<dt>contours<dd>Number of polyline contours.
<dt>isClosed<dd>Indicates whether the polylines must be drawn closed. If closed, the
function draws the line from the last vertex of every contour to the first
vertex.
<dt>color<dd>Polygon color (RGB) or brightness (grayscale image).
<dt>thickness<dd>Thickness of the polyline edges.
<dt>connectivity<dd>The connectivity of polyline segments, 8 (by default) or 4.
</dl><p>
The function <a href="#decl_cvPolyLine">cvPolyLine</a> draws a set of simple or thick polylines.</p>


<hr><h3><a name="decl_cvPolyLineAA">PolyLineAA</a></h3>
<p class="Blurb">Draws antialiased polygons</p>
<pre>
void cvPolyLineAA( CvArr* img, CvPoint** pts, int* npts, int contours,
                   int isClosed, int color, int scale =0);
</pre><p><dl>
<dt>img<dd>Image.
<dt>pts<dd>Array of pointers to polylines.
<dt>npts<dd>Array of polyline vertex counters.
<dt>contours<dd>Number of polyline contours.
<dt>isClosed<dd>Indicates whether the polylines must be drawn closed. If closed, the
function draws the line from the last vertex of every contour to the first
vertex.
<dt>color<dd>Polygon color (RGB) or brightness (grayscale image).
<dt>scale<dd>Specifies number of fractional bits in the coordinates of polyline
vertices.
</dl><p>
The function <a href="#decl_cvPolyLineAA">cvPolyLineAA</a> draws a set of antialiased polylines.</p>


<hr><h3><a name="decl_cvInitFont">InitFont</a></h3>
<p class="Blurb">Initializes font structure</p>
<pre>
void cvInitFont( CvFont* font, CvFontFace fontFace, float hscale,
                 float vscale, float italicScale, int thickness );
</pre><p><dl>
<dt>font<dd>Pointer to the font structure initialized by the function.
<dt>fontFace<dd>Font name identifier. Only the font <code>CV_FONT_VECTOR0</code> is currently
supported.
<dt>hscale<dd>Horizontal scale. If equal to <code>1.0f</code>, the characters have the original
width depending on the font type. If equal to <code>0.5f</code>, the characters are of half
the original width.
<dt>vscale<dd>Vertical scale. If equal to <code>1.0f</code>, the characters have the original
height depending on the font type. If equal to <code>0.5f</code>, the characters are of half
the original height.
<dt>italicScale<dd>Approximate tangent of the character slope relative to the vertical
line. Zero value means a non-italic font, <code>1.0f</code> means <code>&asymp;45&deg;</code> slope, etc.
thickness Thickness of lines composing letters outlines. The function <a href="#decl_cvLine">cvLine</a> is
used for drawing letters.
</dl><p>
The function <a href="#decl_cvInitFont">cvInitFont</a> initializes the font structure that can be passed further
into text drawing functions. Although only one font is supported, it is possible
to get different font flavors by varying the scale parameters, slope, and
thickness.</p>


<hr><h3><a name="decl_cvPutText">PutText</a></h3>
<p class="Blurb">Draws text string</p>
<pre>
void cvPutText( CvArr* img, const char* text, CvPoint org, CvFont* font, int color );
</pre><p><dl>
<dt>img<dd>Input image.
<dt>text<dd>String to print.
<dt>org<dd>Coordinates of the bottom-left corner of the first letter.
<dt>font<dd>Pointer to the font structure.
<dt>color<dd>Text color (RGB) or brightness (grayscale image).
</dl><p>
The function <a href="#decl_cvPutText">cvPutText</a> renders the text in the image with the specified font and
color. The printed text is clipped by ROI rectangle. Symbols that do not belong
to the specified font are replaced with the rectangle symbol.</p>


<hr><h3><a name="decl_cvGetTextSize">GetTextSize</a></h3>
<p class="Blurb">Retrieves width and height of text string</p>
<pre>
void cvGetTextSize( CvFont* font, const char* textString, CvSize* textSize, int* ymin );
</pre><p><dl>
<dt>font<dd>Pointer to the font structure.
<dt>textString<dd>Input string.
<dt>textSize<dd>Resultant size of the text string. Height of the text does not include
the height of character parts that are below the baseline.
<dt>ymin<dd>Lowest y coordinate of the text relative to the baseline. Negative, if the
text includes such characters as g, j, p, q, y, etc., and zero otherwise.
</dl><p>
The function <a href="#decl_cvGetTextSize">cvGetTextSize</a> calculates the binding rectangle for the given text
string when a specified font is used.</p>


<hr><h2><a name="ch2_features">Gradients, Edges and Corners</a></h2>

<hr><h3><a name="decl_cvSobel">Sobel</a></h3>
<p class="Blurb">Calculates first, second, third or mixed image derivatives using extended Sobel operator</p>
<pre>
void cvSobel( const CvArr* I, CvArr* J, int dx, int dy, int apertureSize=3 );
</pre><p><dl>
<dt>I<dd>Source image.
<dt>J<dd>Destination image.
<dt>ox<dd>Order of the derivative x .
<dt>oy<dd>Order of the derivative y .
<dt>apertureSize<dd>Size of the extended Sobel kernel, must be 1, 3, 5 or 7.
In all cases except 1, apertureSize &times; apertureSize separable kernel will be used to calculate
the derivative. For <code>apertureSize</code>=1 3x1 or 1x3 kernel is used (Gaussian smoothing is not done).
There is also special value <code>CV_SCHARR</code> (=-1) that corresponds to 3x3 Scharr filter that may
give more accurate results than 3x3 Sobel. Scharr aperture is:
<pre>
| -3 0  3|
|-10 0 10|
| -3 0  3|
</pre>
for x-derivative or transposed for y-derivative.
</dl><p>
The function <a href="#decl_cvSobel">cvSobel</a> calculates the image derivative by convolving the image
with the appropriate kernel:</p>
<pre>
J(x,y) = d<sup>ox+oy</sup>I/dx<sup>ox</sup>&bull;dy<sup>oy</sup> |<sub>(x,y)</sub>
</pre>
The Sobel operators combine Gaussian smoothing and differentiation so the result is more or less
robust to the noise. Most often, the function is called with (ox=1, oy=0, apertureSize=3) or
(ox=0, oy=1, apertureSize=3) to calculate first x- or y- image derivative.
The first case corresponds to</p>
<pre>
  |-1  0  1|
  |-2  0  2|
  |-1  0  1|
</pre>
<p>kernel and the second one corresponds to</p>
<pre>
  |-1 -2 -1|
  | 0  0  0|
  | 1  2  1|
or
  | 1  2  1|
  | 0  0  0|
  |-1 -2 -1|
</pre>
kernel, depending on the image origin (<code>origin</code> field of <code>IplImage</code> structure).
No scaling is done, so the destination image usually has larger by absolute value numbers than
the source image. To avoid overflow, the function requires 16-bit destination image if
the source image is 8-bit. The result can be converted back to 8-bit using <a href="#decl_cvConvertScale">cvConvertScale</a> or
<a href="OpenCVRef_BasicFuncs.htm#decl_cvConvertScaleAbs">cvConvertScaleAbs</a> functions. Besides 8-bit images the function can process 32-bit floating-point
images. Both source and destination must be single-channel images of equal size or ROI size.
</p>


<hr><h3><a name="decl_cvLaplace">Laplace</a></h3>
<p class="Blurb">Calculates Laplacian of the image</p>
<pre>
void cvLaplace( const CvArr* I, CvArr* J, int apertureSize=3 );
</pre><p><dl>
<dt>I<dd>Source image.
<dt>J<dd>Destination image.
<dt>apertureSize<dd>Aperture parameter for Sobel operator (see <a href="#decl_cvSobel">cvSobel</a>).
</dl><p>
The function <a href="#decl_cvLaplace">cvLaplace</a> calculates Laplacian of the source image by summing
second x- and y- derivatives calcualted using Sobel operator:</p>
<pre>
J(x,y) = d<sup>2</sup>I/dx<sup>2</sup> + d<sup>2</sup>I/dy<sup>2</sup>
</pre>
<p>
Specifying <code>apertureSize</code>=1 gives the fastest variant that is equal to
convolving the image with the following kernel:</p>
<pre>
|0  1  0|
|1 -4  1|
|0  1  0|
</pre><p>
As well as in <a href="#decl_cvSobel">cvSobel</a> function, no scaling is done and the same combinations of input and
output formats are supported.
</p>


<hr><h3><a name="decl_cvCanny">Canny</a></h3>
<p class="Blurb">Implements Canny algorithm for edge detection</p>
<pre>
void cvCanny( const CvArr* img, CvArr* edges, double threshold1,
              double threshold2, int apertureSize=3 );
</pre><p><dl>
<dt>img<dd>Input image.
<dt>edges<dd>Image to store the edges found by the function.
<dt>threshold1<dd>The first threshold.
<dt>threshold2<dd>The second threshold.
<dt>apertureSize<dd>Aperture parameter for Sobel operator (see <a href="#decl_cvSobel">cvSobel</a>).
</dl><p>
The function <a href="#decl_cvCanny">cvCanny</a> finds the edges on the input image <code>img</code> and marks them in the
output image <code>edges</code> using the Canny algorithm. The smallest of <code>threshold1</code> and
<code>threshold2</code> is used for edge linking, the largest - to find initial segments of strong edges.</p>


<hr><h3><a name="decl_cvPreCornerDetect">PreCornerDetect</a></h3>
<p class="Blurb">Calculates two constraint images for corner detection</p>
<pre>
void cvPreCornerDetect( const CvArr* img, CvArr* corners, int apertureSize=3 );
</pre><p><dl>
<dt>img<dd>Input image.
<dt>corners<dd>Image to store the corner candidates.
<dt>apertureSize<dd>Aperture parameter for Sobel operator (see <a href="#decl_cvSobel">cvSobel</a>).
</dl><p>
The function <a href="#decl_cvPreCornerDetect">cvPreCornerDetect</a> finds the corners on the input image <code>img</code> and stores
them in the <code>corners</code> image in accordance with <code>Method 1</code> for corner detection desctibed
in the guide.</p>


<hr><h3><a name="decl_cvCornerEigenValsAndVecs">CornerEigenValsAndVecs</a></h3>
<p class="Blurb">Calculates eigenvalues and eigenvectors of image blocks for corner detection</p>
<pre>
void cvCornerEigenValsAndVecs( const CvArr* I, CvArr* eigenvv,
                               int blockSize, int apertureSize=3 );
</pre><p><dl>
<dt>I<dd>Input image.
<dt>eigenvv<dd>Image to store the results. It must be 6 times wider than the input image.
<dt>blockSize<dd>Neighborhood size (see discussion).
<dt>apertureSize<dd>Aperture parameter for Sobel operator (see <a href="#decl_cvSobel">cvSobel</a>).
</dl><p>
For every pixel the function <code>cvCornerEigenValsAndVecs</code> considers
<code>blockSize</code> &times; <code>blockSize</code> neigborhood S(p). It calcualtes
covariation matrix of derivatives over the neigborhood as:</p>
<pre>
    | sum<sub>S(p)</sub>(dI/dx)<sup>2</sup>   sum<sub>S(p)</sub>(dI/dx&bull;dI/dy)|
M = |                                 |
    | sum<sub>S(p)</sub>(dI/dx&bull;dI/dy)  sum<sub>S(p)</sub>(dI/dy)<sup>2</sup> |
</pre><p>
After that it finds eigenvectors and eigenvalues of the resultant matrix and stores
them into destination image in form
(&lambda;<sub>1</sub>, &lambda;<sub>2</sub>, x<sub>1</sub>, y<sub>1</sub>, x<sub>2</sub>, y<sub>2</sub>),
where<br>
&lambda;<sub>1</sub>, &lambda;<sub>2</sub> - eigenvalues of <code>M</code>; not sorted<br>
(x<sub>1</sub>, y<sub>1</sub>) - eigenvector corresponding to &lambda;<sub>1</sub><br>
(x<sub>2</sub>, y<sub>2</sub>) - eigenvector corresponding to &lambda;<sub>2</sub><br>
</p>


<hr><h3><a name="decl_cvCornerMinEigenVal">CornerMinEigenVal</a></h3>
<p class="Blurb">Calculates minimal eigenvalue of image blocks for corner detection</p>
<pre>
void cvCornerMinEigenVal( const CvArr* img, CvArr* eigenvv, int blockSize, int apertureSize=3 );
</pre><p><dl>
<dt>img<dd>Input image.
<dt>eigenvv<dd>Image to store the minimal eigen values. Should have the same size as <code>img</code>
<dt>blockSize<dd>Neighborhood size (see discussion of <a href="#decl_cvCornerEigenValsAndVecs">cvCornerEigenValsAndVecs</a>).
<dt>apertureSize<dd>Aperture parameter for Sobel operator (see <a href="#decl_cvSobel">cvSobel</a>).
format. In the case of floating-point input format this parameter is the number
of the fixed float filter used for differencing.
</dl><p>
The function <a href="#decl_cvCornerMinEigenVal">cvCornerMinEigenVal</a> is similar to <a href="#decl_cvCornerEigenValsAndVecs">cvCornerEigenValsAndVecs</a> but
it calculates and stores only the minimal eigen value of derivative covariation matrix for every pixel,
i.e. min(&lambda;<sub>1</sub>, &lambda;<sub>2</sub>) in terms of the previous function.
</p>


<hr><h3><a name="decl_cvFindCornerSubPix">FindCornerSubPix</a></h3>
<p class="Blurb">Refines corner locations</p>
<pre>
void cvFindCornerSubPix( IplImage* I, CvPoint2D32f* corners,
                         int count, CvSize win, CvSize zeroZone,
                         CvTermCriteria criteria );
</pre><p><dl>
<dt>I<dd>Input image.
<dt>corners<dd>Initial coordinates of the input corners and refined coordinates on
output.
<dt>count<dd>Number of corners.
<dt>win<dd>Half sizes of the search window. For example, if <code>win</code>=(5,5) then
5*2+1 &times; 5*2+1 = 11 &times; 11 search window is used.
<dt>zeroZone<dd>Half size of the dead region in the middle of the search zone over which the
summation in formulae below is not done. It is used sometimes to avoid possible singularities of the autocorrelation matrix.
The value of (-1,-1) indicates that there is no such size.
<dt>criteria<dd>Criteria for termination of the iterative process of corner refinement.
That is, the process of corner position refinement stops either after certain number of iteration or
when a required accuracy is achieved. The <code>criteria</code> may specify either of or both the maximum
number of iteration and the required accuracy.
</dl><p>
The function <a href="#decl_cvFindCornerSubPix">cvFindCornerSubPix</a> iterates to find the sub-pixel accurate location
of a corner, or "radial saddle point", as shown in on the picture below.</p>
<p>
<img align="center" src="pics/cornersubpix.png">
</p>
<p>
Sub-pixel accurate corner (radial saddle point) locator is based on the
observation that any vector from <code>q</code> to <code>p</code> is orthogonal to the image gradient.
</p>
<p>
The core idea of this algorithm is based on the observation that every vector
from the center <code>q</code> to a point <code>p</code> located within a neighborhood of <code>q</code> is orthogonal
to the image gradient at <code>p</code> subject to image and measurement noise. Thus:
</p>
<pre>
&epsilon;<sub>i</sub>=DI<sub>p<sub>i</sub></sub><sup>T</sup>&bull;(q-p<sub>i</sub>)
</pre>
where <code>DI<sub>p<sub>i</sub></sub></code> is the image gradient
at the one of the points <code>p<sub>i</sub></code> in a neighborhood of <code>q</code> .
The value of <code>q</code> is to be found such that <code>&epsilon;<sub>i</sub></code> is minimized.
A system of equations may be set up with <code>&epsilon;<sub>i</sub></code>' set to zero:</p>
<pre>
sum<sub>i</sub>(DI<sub>p<sub>i</sub></sub>&bull;DI<sub>p<sub>i</sub></sub><sup>T</sup>)&bull;q - sum<sub>i</sub>(DI<sub>p<sub>i</sub></sub>&bull;DI<sub>p<sub>i</sub></sub><sup>T</sup>&bull;p<sub>i</sub>) = 0
</pre>
<p>where the gradients are summed within a neighborhood ("search window") of <code>q</code>.
Calling the first gradient term <code>G</code> and the second gradient term <code>b</code> gives:</p>
<pre>
q=G<sup>-1</sup>&bull;b
</pre>
<p>
The algorithm sets the center of the neighborhood window at this new center <code>q</code>
and then iterates until the center keeps within a set threshold.
</p>


<hr><h3><a name="decl_cvGoodFeaturesToTrack">GoodFeaturesToTrack</a></h3>
<p class="Blurb">Determines strong corners on image</p>
<pre>
void cvGoodFeaturesToTrack( IplImage* image, IplImage* eigImage, IplImage* tempImage,
                            CvPoint2D32f* corners, int* cornerCount,
                            double qualityLevel, double minDistance );
</pre><p><dl>
<dt>image<dd>The source 8-bit or floating-point 32-bit, single-channel image.
<dt>eigImage<dd>Temporary floating-point 32-bit image of the same size as <code>image</code>.
<dt>tempImage<dd>Another temporary image of the same size and same format as <code>eigImage</code>.
<dt>corners<dd>Output parameter. Detected corners.
<dt>cornerCount<dd>Output parameter. Number of detected corners.
<dt>qualityLevel<dd>Multiplier for the maxmin eigenvalue; specifies minimal accepted
quality of image corners.
<dt>minDistance<dd>Limit, specifying minimum possible distance between returned
corners; Euclidian distance is used.
</dl><p>
The function <a href="#decl_cvGoodFeaturesToTrack">cvGoodFeaturesToTrack</a> finds corners with big eigenvalues in the
image. The function first calculates the minimal eigenvalue for every source image pixel
using <a href="#decl_cvCornerMinEigenVal">cvCornerMinEigenVal</a> function and stores them in <code>eigImage</code>.
Then it performs non-maxima suppression (only local maxima in 3x3 neighborhood remain).
The next step is rejecting the corners with the
minimal eigenvalue less than <code>qualityLevel</code>&bull;max(<code>eigImage</code>(x,y)). Finally,
the function ensures that all the corners found are distanced enough from one
another by considering the corners (the most strongest corners are considered first)
and checking that the distance between the newly considered feature and the features considered earlier
is larger than <code>minDistance</code>. So, the function removes the features than are too close
to the stronger features.</p>


<hr><h2><a name="ch2_geometry">Sampling, Interpolation and Geometrical Transforms</a></h2>

<hr><h3><a name="decl_cvInitLineIterator">InitLineIterator</a></h3>
<p class="Blurb">Initializes line iterator</p>
<pre>
int cvInitLineIterator( const CvArr* img, CvPoint pt1, CvPoint pt2,
                        CvLineIterator* lineIterator, int connectivity=8 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pt1<dd>Starting the line point.
<dt>pt2<dd>Ending the line point.
<dt>lineIterator<dd>Pointer to the line iterator state structure.
<dt>connectivity<dd>The scanned line connectivity, 4 or 8.
</dl><p>
The function <a href="#decl_cvInitLineIterator">cvInitLineIterator</a> initializes the line iterator and returns the
number of pixels between two end points. Both points must be inside the image.
After the iterator has been initialized, all the points on the raster line that
connects the two ending points may be retrieved by successive calls of
<code>CV_NEXT_LINE_POINT</code> point. The points on the line are calculated one by one using
4-connected or 8-connected Bresenham algorithm.</p>
<h4>Example. Using line iterator to calculate pixel values along the color line</h4>
<pre>
    CvScalar sum_line_pixels( IplImage* img, CvPoint pt1, CvPoint pt2 )
    {
        CvLineIterator iterator;
        int blue_sum = 0, green_sum = 0, red_sum = 0;
        int count = cvInitLineIterator( img, pt1, pt2, &iterator, 8 );

        for( int i = 0; i &lt; count; i++ ){
            blue_sum += iterator.ptr[0];
            green_sum += iterator.ptr[1];
            red_sum += iterator.ptr[2];
            CV_NEXT_LINE_POINT(iterator);

            /* print the pixel coordinates: demonstrates how to calculate the coordinates */
            {
            int offset, x, y;
            /* assume that ROI is not set, otherwise need to take it into account. */
            offset = iterator.ptr - (uchar*)(img->imageData);
            y = offset/img->widthStep;
            x = (offset - y*img->widthStep)/(3*sizeof(uchar) /* size of pixel */);
            printf("(%d,%d)\n", x, y );
            }
        }
        return cvScalar( blue_sum, green_sum, red_sum );
    }
</pre>


<hr><h3><a name="decl_cvSampleLine">SampleLine</a></h3>
<p class="Blurb">Reads raster line to buffer</p>
<pre>
int cvSampleLine( const CvArr* img, CvPoint pt1, CvPoint pt2,
                  void* buffer, int connectivity=8 );
</pre><p><dl>
<dt>img<dd>Image.
<dt>pt1<dd>Starting the line point.
<dt>pt2<dd>Ending the line point.
<dt>buffer<dd>Buffer to store the line points; must have enough size to store
max( |<code>pt2.x</code>-<code>pt1.x</code>|+1, |<code>pt2.y</code>-<code>pt1.y</code>|+1 )</code> points in case
of 8-connected line and |<code>pt2.x</code>-<code>pt1.x</code>|+|<code>pt2.y</code>-<code>pt1.y</code>|+1 in case
of 4-connected line.
<dt>connectivity<dd>The line connectivity, 4 or 8.
</dl><p>
The function <a href="#decl_cvSampleLine">cvSampleLine</a> implements a particular case of application of line
iterators. The function reads all the image points lying on the line between <code>pt1</code>
and <code>pt2</code>, including the ending points, and stores them into the buffer.</p>


<hr><h3><a name="decl_cvGetRectSubPix">GetRectSubPix</a></h3>
<p class="Blurb">Retrieves pixel rectangle from image with sub-pixel accuracy</p>
<pre>
void cvGetRectSubPix( const CvArr* I, CvArr* J, CvPoint2D32f center );
</pre><p><dl>
<dt>I<dd>Source image.
<dt>J<dd>Extracted rectangle.
<dt>center<dd>Floating point coordinates of the extracted rectangle center within the source image.
The center must be inside the image.
</dl><p>
The function <a href="#decl_cvGetRectSubPix">cvGetRectSubPix</a> extracts pixels from <code>I</code>:</p>
<pre>
J( x+width(J)/2, y+height(J)/2 )=I( x+center.x, y+center.y )
</pre>
<p>
where the values of pixels at non-integer coordinates ( x+center.x, y+center.y ) are
retrieved using bilinear interpolation. Every channel of multiple-channel images is processed
independently.
Whereas the rectangle center must be inside the image, the whole rectangle may be partially occluded.
In this case, the replication border mode is used to get pixel values beyond the image boundaries.
</p>


<hr><h3><a name="decl_cvGetQuadrangeSubPix">GetQuadrangeSubPix</a></h3>
<p class="Blurb">Retrieves pixel quadrangle from image with sub-pixel accuracy</p>
<pre>
void cvGetQuadrangeSubPix( const CvArr* I, CvArr* J, const CvArr* M,
                           int fillOutliers=0, CvScalar fillValue=cvScalarAll(0) );
</pre><p><dl>
<dt>I<dd>Source image.
<dt>J<dd>Extracted quadrangle.
<dt>M<dd>The transformation 3 &times; 2 matrix [<code>A</code>|<code>b</code>] (see the discussion).
<dt>fillOutliers<dd>The flag indicating whether to interpolate values of pixel taken from outside of the
source image using replication mode (<code>fillOutliers</code>=0) or set them a fixed value (<code>fillOutliers</code>=1).
<dt>fillValue<dd>The fixed value to set the outlier pixels to if <code>fillOutliers</code>=1.
</dl><p>
The function <a href="#decl_cvGetQuadrangleSubPix">cvGetQuadrangleSubPix</a> extracts pixels from <code>I</code> at sub-pixel accuracy
and stores them to <code>J</code> as follows:</p>
<pre>
J( x+width(J)/2, y+height(J)/2 )= I( A<sub>11</sub>x+A<sub>12</sub>y+b<sub>1</sub>, A<sub>21</sub>x+A<sub>22</sub>y+b<sub>2</sub> ),

where <code>A</code> and <code>b</code> are taken from <code>M</code>
    | A<sub>11</sub> A<sub>12</sub>  b<sub>1</sub> |
M = |            |
    | A<sub>21</sub> A<sub>22</sub>  b<sub>2</sub> |
</pre>
<p>
where the values of pixels at non-integer coordinates A&bull;(x,y)<sup>T</sup>+b are
retrieved using bilinear interpolation. Every channel of multiple-channel images is processed
independently.</p>
<h4>Example. Using cvGetQuadrangeSubPix for image rotation.</h4>
<pre>
#include "cv.h"
#include "highgui.h"
#include "math.h"

int main( int argc, char** argv )
{
    IplImage* src;
    /* the first command line parameter must be image file name */
    if( argc==2 && (src = cvLoadImage(argv[1], -1))!=0)
    {
        IplImage* dst = cvCloneImage( src );
        int delta = 1;
        int angle = 0;

        cvNamedWindow( "src", 1 );
        cvShowImage( "src", src );

        for(;;)
        {
            float m[6];
            double factor = (cos(angle*CV_PI/180.) + 1.1)*3;
            CvMat M = cvMat( 2, 3, CV_32F, m );
            int w = src->width;
            int h = src->height;

            m[0] = (float)(factor*cos(-angle*2*CV_PI/180.));
            m[1] = (float)(factor*sin(-angle*2*CV_PI/180.));
            m[2] = w*0.5f;
            m[3] = -m[1];
            m[4] = m[0];
            m[5] = h*0.5f;

            cvGetQuadrangleSubPix( src, dst, &M, 1, cvScalarAll(0));

            cvNamedWindow( "dst", 1 );
            cvShowImage( "dst", dst );

            if( cvWaitKey(5) == 27 )
                break;

            angle = (angle + delta) % 360;
        }
    }
    return 0;
}
</pre>


<hr><h3><a name="decl_cvResize">Resize</a></h3>
<p class="Blurb">Resizes image</p>
<pre>
void cvResize( const CvArr* I, CvArr* J, int interpolation=CV_INTER_LINEAR );
</pre><p><dl>
<dt>I<dd>Source image.
<dt>J<dd>Destination image.
<dt>interpolation<dd>Interpolation method:<ul>
    <li>CV_INTER_NN - nearest-neigbor interpolation,
    <li>CV_INTER_LINEAR - bilinear interpolation (used by default)
    </ul>
</dl><p>
The function <a href="#decl_cvResize">cvResize</a> resizes image <code>I</code> so that it fits exactly to <code>J</code>.
If ROI is set, the function consideres the ROI as supported as usual.
 the source image using the specified structuring element B
that determines the shape of a pixel neighborhood over which the minimum is taken:</p>
<pre>
C=erode(A,B): C(I)=min<sub>(K in B<sub>I</sub>)</sub>A(K)
</pre>
<p>The function supports the in-place mode when
the source and destination pointers are the same. Erosion can be applied several
times <code>iterations</code> parameter. Erosion on a color image means independent
transformation of all the channels.</p>



<hr><h2><a name="ch2_morphology">Morphological Operations</a></h2>

<hr><h3><a name="decl_cvCreateStructuringElementEx">CreateStructuringElementEx</a></h3>
<p class="Blurb">Creates structuring element</p>
<pre>
IplConvKernel* cvCreateStructuringElementEx( int nCols, int nRows, int anchorX, int anchorY,
                                             CvElementShape shape, int* values );
</pre><p><dl>
<dt>nCols<dd>Number of columns in the structuring element.
<dt>nRows<dd>Number of rows in the structuring element.
<dt>anchorX<dd>Relative horizontal offset of the anchor point.
<dt>anchorY<dd>Relative vertical offset of the anchor point.
<dt>shape<dd>Shape of the structuring element; may have the following values:
<ul>
<li><code>CV_SHAPE_RECT</code> , a rectangular element;
<li><code>CV_SHAPE_CROSS</code> , a cross-shaped element;
<li><code>CV_SHAPE_ELLIPSE</code> , an elliptic element;
<li><code>CV_SHAPE_CUSTOM</code> , a user-defined element. In this case the parameter <code> values</code>
  specifies the mask, that is, which neighbors of the pixel must be considered.
</ul>
<dt>values<dd> Pointer to the structuring element data, a plane array, representing
row-by-row scanning of the element matrix. Non-zero values indicate points that
belong to the element. If the pointer is <code> NULL</code> , then all values are considered
non-zero, that is, the element is of a rectangular shape. This parameter is
considered only if the shape is <code> CV_SHAPE_CUSTOM</code>  .
</dl><p>
The function <a href="#decl_cv CreateStructuringElementEx">cv CreateStructuringElementEx</a>  allocates and fills the structure
<code> IplConvKernel</code> , which can be used as a structuring element in the morphological
operations.</p>


<hr><h3><a name="decl_cvReleaseStructuringElement">ReleaseStructuringElement</a></h3>
<p class="Blurb">Deletes structuring element</p>
<pre>
void cvReleaseStructuringElement( IplConvKernel** ppElement );
</pre><p><dl>
<dt>ppElement<dd>Pointer to the deleted structuring element.
</dl><p>
The function <a href="#decl_cv ReleaseStructuringElement">cv ReleaseStructuringElement</a>  releases the structure <code> IplConvKernel</code>  that
is no longer needed. If <code> *ppElement</code>  is <code> NULL</code> , the function has no effect. The
function returns created structuring element.</p>


<hr><h3><a name="decl_cvErode">Erode</a></h3>
<p class="Blurb">Erodes image by using arbitrary structuring element</p>
<pre>
void cvErode( const CvArr* A, CvArr* C, IplConvKernel* B=0, int iterations=1 );
</pre><p><dl>
<dt>A<dd>Source image.
<dt>C<dd>Destination image.
<dt>B<dd>Structuring element used for erosion. If it is <code>NULL</code>, a 3&times;3 rectangular
structuring element is used.
<dt>iterations<dd>Number of times erosion is applied.
</dl><p>
The function <a href="#decl_cvErode">cvErode</a> erodes the source image using the specified structuring element B
that determines the shape of a pixel neighborhood over which the minimum is taken:</p>
<pre>
C=erode(A,B):  C(x,y)=min<sub>((x',y') in B<sub>(x,y)</sub>)</sub>A(x',y')
</pre>
<p>The function supports the in-place mode when
the source and destination pointers are the same. Erosion can be applied several
times <code>iterations</code> parameter. Erosion on a color image means independent
transformation of all the channels.</p>


<hr><h3><a name="decl_cvDilate">Dilate</a></h3>
<p class="Blurb">Dilates image by using arbitrary structuring element</p>
<pre>
void cvDilate( const CvArr* A, CvArr* C, IplConvKernel* B=0, int iterations=1 );
</pre><p><dl>
<dt>A<dd>Source image.
<dt>C<dd>Destination image.
<dt>B<dd>Structuring element used for erosion. If it is <code>NULL</code>, a 3&times;3 rectangular
structuring element is used.
<dt>iterations<dd>Number of times erosion is applied.
</dl><p>
The function <a href="#decl_cvDilate">cvDilate</a> dilates the source image using the specified structuring element B
that determines the shape of a pixel neighborhood over which the maximum is taken:</p>
<pre>
C=dilate(A,B):  C(x,y)=max<sub>((x',y') in B<sub>(x,y)</sub>)</sub>A(x',y')
</pre>
<p>The function supports the in-place mode when
the source and destination pointers are the same. Dilation can be applied several
times <code>iterations</code> parameter. Dilation on a color image means independent
transformation of all the channels.</p>


<hr><h3><a name="decl_cvMorphologyEx">MorphologyEx</a></h3>
<p class="Blurb">Performs advanced morphological transformations</p>
<pre>
void cvMorphologyEx( const CvArr* A, CvArr* C, CvArr* temp,
                     IplConvKernel* B, CvMorphOp op, int iterations );
</pre><p><dl>
<dt>A<dd>Source image.
<dt>C<dd>Destination image.
<dt>temp<dd>Temporary image, required in some cases.
<dt>B<dd>Structuring element.
<dt>op<dd>Type of morphological operation (see the discussion).
<dt>iterations<dd>Number of times erosion and dilation are applied.
</dl><p>
The function <a href="#decl_cvMorphologyEx">cvMorphologyEx</a> performs advanced morphological transformations using
on erosion and dilation as basic operations.</p>
<pre>
<code>Opening:</code>
C=open(A,B)=dilate(erode(A,B),B),   if op=CV_MOP_OPEN

<code>Closing:</code>
C=close(A,B)=erode(dilate(A,B),B),  if op=CV_MOP_CLOSE

<code>Morphological gradient:</code>
C=morph_grad(A,B)=dilate(A,B)-erode(A,B),  if op=CV_MOP_GRADIENT

<code>"Top hat":</code>
C=tophat(A,B)=A-erode(A,B),   if op=CV_MOP_TOPHAT

<code>"Black hat":</code>
C=blackhat(A,B)=dilate(A,B)-A,   if op=CV_MOP_BLACKHAT
</pre>
<p>
The temporary image <code>temp</code> is required if <code>op=CV_MOP_GRADIENT</code> or if <code>A=C</code>
(inplace operation) and <code>op=CV_MOP_TOPHAT</code> or <code>op=CV_MOP_BLACKHAT</code>
</p>


<hr><h2><a name="ch2_filters">Filters and Color Conversion</a></h2>

<hr><h3><a name="decl_cvSmooth">Smooth</a></h3>
<p class="Blurb">Smooths the image in one of several ways</p>
<pre>
void cvSmooth( const CvArr* src, CvArr* dst,
               int smoothtype=CV_GAUSSIAN,
               int param1=3, int param2=0 );
</pre><p><dl>
<dt>src<dd>The source image.
<dt>dst<dd>The destination image.
<dt>smoothtype<dd>Type of the smoothing:<ul>
<li>CV_BLUR_NO_SCALE (simple blur with no scaling) -
              summation over a pixel <code>param1</code>&times;<code>param2</code> neighborhood.
              If the neighborhood size is not fixed, one may use <a href="#decl_cvIntegral">cvIntegral</a> function.
<li>CV_BLUR (simple blur) - summation over a pixel <code>param1</code>&times;<code>param2</code> neighborhood with
             subsequent scaling by 1/(<code>param1</code>&bull;<code>param2</code>).
<li>CV_GAUSSIAN (gaussian blur) - convolving image with <code>param1</code>&times;<code>param2</code> Gaussian.
<li>CV_MEDIAN (median blur) - finding median of <code>param1</code>&times;<code>param1</code> neighborhood (i.e.
                              the neighborhood is square).
<li>CV_BILATERAL (bilateral filter) - applying bilateral 3x3 filtering with color sigma=<code>param1</code> and
                                      space sigma=<code>param2</code>. Information about bilateral filtering
                                      can be found at <a href="http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html">
                                                      http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html</a>
</ul>
<dt>param1<dd>The first parameter of smoothing operation.
<dt>param2<dd>The second parameter of smoothing operation. In case of simple scaled/non-scaled and
Gaussian blur if <code>param2</code> is zero, it is set to <code>param1</code>.
</dl><p>
The function <a href="#decl_cvSmooth">cvSmooth</a> smooths image using one of several methods. Every of the methods
has some features and restrictions listed below</p>
<p>Blur with no scaling works with single-channel images only and supports accumulation of
8-bit to 16-bit format (similar to <a href="#decl_cvSobel">cvSobel</a> and <a href="#decl_cvLaplace">cvLaplace</a>) and 32-bit floating point
to 32-bit floating-point format.</p><p>
Simple blur and Gaussian blur support 1- or 3-channel, 8-bit and 32-bit floating point images.
These two methods can process images in-place.</p>
<p>Median and bilateral filters work with 1- or 3-channel 8-bit images and can not process images
in-place.</p>


<hr><h3><a name="decl_cvIntegral">Integral</a></h3>
<p class="Blurb">Calculates integral images</p>
<pre>
void cvIntegral( const CvArr* I, CvArr* S, CvArr* Sq=0, CvArr* T=0 );
</pre><p><dl>
<dt>I<dd>The source image, <code>w</code>&times;<code>h</code>, single-channel, 8-bit, or floating-point (32f or 64f).
<dt>S<dd>The sum image, <code>w+1</code>&times;<code>h+1</code>, single-channel, 32-bit integer or double precision floating-point (64f).
<dt>Sq<dd>The square sum image, <code>w+1</code>&times;<code>h+1</code>, single-channel, double precision floating-point (64f).
<dt>T<dd>The tilted sum image (sum of rotated by 45&deg; image), <code>w+1</code>&times;<code>h+1</code>, single-channel, the same data type as <code>sum</code>.
</dl><p>
The function <a href="#decl_cvIntegral">cvIntegral</a> calculates one or more integral images for the source image as following:</p>
<pre>
S(X,Y)=sum<sub>x&lt;X,y&lt;Y</sub>I(x,y)

Sq(X,Y)=sum<sub>x&lt;X,y&lt;Y</sub>I(x,y)<sup>2</sup>

T(X,Y)=sum<sub>y&lt;Y,abs(x-X)&lt;y</sub>I(x,y)
</pre>
<p>After that the images are calculated, they can be used to calculate sums of pixels over an arbitrary
rectangles, for example:</p>
<pre>
sum<sub>x1&lt;=x&lt;x2,y1&lt;=y&lt;y2</sub>I(x,y)=S(x2,y2)-S(x1,y2)-S(x2,y1)+S(x1,x1)
</pre>
<p>It makes possible to do a fast blurring or fast block correlation with variable window size etc.
</p>

<hr><h3><a name="decl_cvCvtColor">CvtColor</a></h3>
<p class="Blurb">Converts image from one color space to another</p>
<pre>
void cvCvtColor( const CvArr* src, CvArr* dst, int code );
</pre><p><dl>
<dt>src<dd>The source 8-bit image.
<dt>dst<dd>The destination 8-bit image.
<dt>code<dd>Color conversion operation that can be specifed using
CV_&lt;src_color_space&gt;2&lt;dst_color_space&gt; constants (see below).
</dl><p>
The function <a href="#decl_cvCvtColor">cvCvtColor</a> converts input image from one color space to another.
The function ignores <code>colorModel</code> and <code>channelSeq</code> fields of <code>IplImage</code> header,
so the source image color space should be specified correctly (including order of the channels in case
of RGB space, e.g. BGR means 24-bit format with B<sub>0</sub> G<sub>0</sub> R<sub>0</sub> B<sub>1</sub> G<sub>1</sub> R<sub>1</sub> ... layout,
whereas RGB means 24-format with R<sub>0</sub> G<sub>0</sub> B<sub>0</sub> R<sub>1</sub> G<sub>1</sub> B<sub>1</sub> ... layout).
The function can do the following transformations:<ul>
<li>Transformations within RGB space like adding/removing alpha channel, reversing the channel order,
conversion to/from 16-bit (Rx5:Gx6:Rx5) color, as well as conversion to/from grayscale using:
<pre>
RGB[A]->Gray: Y=0.212671*R + 0.715160*G + 0.072169*B + 0*A
Gray->RGB[A]: R=Y G=Y B=Y A=0
</pre>
All the possible combinations of input and output format (except equal) are allowed here.
<p></p>
<li>RGB&lt;=&gt;XYZ (CV_BGR2XYZ, CV_RGB2XYZ, CV_XYZ2BGR, CV_XYZ2RGB):
<pre>
|X|   |0.412411  0.357585  0.180454| |R|
|Y| = |0.212649  0.715169  0.072182|*|G|
|Z|   |0.019332  0.119195  0.950390| |B|

|R|   | 3.240479  -1.53715  -0.498535| |X|
|G| = |-0.969256   1.875991  0.041556|*|Y|
|B|   | 0.055648  -0.204043  1.057311| |Z|
</pre>
<p></p>
<li>RGB&lt;=&gt;YCrCb (CV_BGR2YCrCb, CV_RGB2YCrCb, CV_YCrCb2BGR, CV_YCrCb2RGB)
<pre>
Y=0.299*R + 0.587*G + 0.114*B
Cr=(R-Y)*0.713 + 128
Cb=(B-Y)*0.564 + 128

R=Y + 1.403*(Cr - 128)
G=Y - 0.344*(Cr - 128) - 0.714*(Cb - 128)
B=Y + 1.773*(Cb - 128)
</pre>
<p></p>
<li>RGB=>HSV (CV_BGR2HSV,CV_RGB2HSV)
<pre>
V=max(R,G,B)
S=(V-min(R,G,B))*255/V   if V!=0, 0 otherwise

       (G - B)*60/S,  if V=R
H= 180+(B - R)*60/S,  if V=G
   240+(R - G)*60/S,  if V=B

if H&lt;0 then H=H+360
</pre>
<p>
The hue values calcualted using the above formulae vary from 0&deg; to 360&deg; so they are divided by 2 to
fit into 8-bit destination format.
</p><p></p>
<li>RGB=>Lab (CV_BGR2Lab, CV_RGB2Lab)
<pre>
|X|   |0.433910  0.376220  0.189860| |R/255|
|Y| = |0.212649  0.715169  0.072182|*|G/255|
|Z|   |0.017756  0.109478  0.872915| |B/255|

L = 116*Y<sup>1/3</sup>      for Y>0.008856
L = 903.3*Y      for Y&lt;=0.008856

a = 500*(f(X)-f(Y))
b = 200*(f(Y)-f(Z))
where f(t)=t<sup>1/3</sup>              for t>0.008856
      f(t)=7.787*t+16/116   for t&lt;=0.008856
</pre>
The above formulae have been taken from
<a href="http://www.cica.indiana.edu/cica/faq/color_spaces/color.spaces.html">
http://www.cica.indiana.edu/cica/faq/color_spaces/color.spaces.html</a>
<p></p>
<li>Bayer=>RGB (CV_BayerBG2BGR, CV_BayerGB2BGR, CV_BayerRG2BGR, CV_BayerGR2BGR,<br>
                CV_BayerBG2RGB, CV_BayerRG2BGR, CV_BayerGB2RGB, CV_BayerGR2BGR,<br>
                CV_BayerRG2RGB, CV_BayerBG2BGR, CV_BayerGR2RGB, CV_BayerGB2BGR)
<p>Bayer pattern is widely used in CCD and CMOS cameras. It allows to get color picture
out of a single plane where R,G and B pixels (sensors of a particular component) are interleaved like
this:</p>
<p>
<table border=0 width=400>
<tr>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
</tr><tr>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
</tr><tr>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
</tr><tr>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
</tr><tr>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#ff0000"><p align="center">R</font></td>
</tr><tr>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
<td><font size=5 color="#0000ff"><p align="center">B</font></td>
<td><font size=5 color="#008000"><p align="center">G</font></td>
</tr>
</table>
</p><p>
The output RGB components of a pixel are interpolated from 1, 2 or 4 neighbors of the pixel
having the same color. There are several modifications of the above pattern that can be achieved
by shifting the pattern one pixel left and/or one pixel up.
The two letters C<sub>1</sub> and C<sub>2</sub> in the conversion constants CV_BayerC<sub>1</sub>C<sub>2</sub>2{BGR|RGB}
indicate the particular pattern type -
these are components from the second row, second and third columns, respectively.
For example, the above pattern has very popular "BG" type.</p>
</ul>
</p>


<hr><h3><a name="decl_cvThreshold">Threshold</a></h3>
<p class="Blurb">Applies fixed-level threshold to array elements</p>
<pre>
void cvThreshold( const CvArr* src, CvArr* dst, double threshold,
                  double maxValue, int thresholdType );
</pre><p><dl>
<dt>src<dd>Source array (single-channel, 8-bit of 32-bit floating point).
<dt>dst<dd>Destination array; must be either the same type as <code>src</code> or 8-bit.
<dt>threshold<dd>Threshold value.
<dt>maxValue<dd>Maximum value to use with <code>CV_THRESH_BINARY</code>, <code>CV_THRESH_BINARY_INV</code>,
and <code>CV_THRESH_TRUNC</code> thresholding types.
<dt>thresholdType<dd>Thresholding type (see the discussion)
</dl><p>
The function <a href="#decl_cvThreshold">cvThreshold</a> applies fixed-level thresholding to single-channel array.
The function is typically used to get bi-level (binary) image out of grayscale image or
for removing a noise, i.e. filtering out pixels with too small or too large values.
There are several types of thresholding the function supports that are determined by <code>thresholdType</code>:</p>
<pre>
thresholdType=<code>CV_THRESH_BINARY</code>:
dst(x,y) = maxValue, if src(x,y)&gt;threshold
           0, otherwise

thresholdType=<code>CV_THRESH_BINARY_INV</code>:
dst(x,y) = 0, if src(x,y)&gt;threshold
           maxValue, otherwise

thresholdType=<code>CV_THRESH_TRUNC</code>:
dst(x,y) = threshold, if src(x,y)&gt;threshold
           src(x,y), otherwise

thresholdType=<code>CV_THRESH_TOZERO</code>:
dst(x,y) = src(x,y), if (x,y)&gt;threshold
           0, otherwise

thresholdType=<code>CV_THRESH_TOZERO_INV</code>:
dst(x,y) = 0, if src(x,y)&gt;threshold
           src(x,y), otherwise
</pre>
<p>And this is the visual description of thresholding types:</p>
<p>
<img align="center" src="pics/threshold.png">
</p>
</p>

<hr><h3><a name="decl_cvAdaptiveThreshold">AdaptiveThreshold</a></h3>
<p class="Blurb">Applies adaptive threshold to array</p>
<pre>
void cvAdaptiveThreshold( const CvArr* src, CvArr* dst, double maxValue,
                          int adaptiveMethod, int thresholdType,
                          int blockSize, double param1 );
</pre><p><dl>
<dt>src<dd>Source image.
<dt>dst<dd>Destination image.
<dt>maxValue<dd>Maximum value that is used with <code>CV_THRESH_BINARY</code> and <code>CV_THRESH_BINARY_INV</code>.
<dt>adaptiveMethod<dd>Adaptive thresholding algorithm to use: <code>CV_ADAPTIVE_THRESH_MEAN_C</code>
or <code>CV_ADAPTIVE_THRESH_GAUSSIAN_C</code> (see the discussion).
<dt>thresholdType<dd>Thresholding type; must be one of
<ul>
<li><code>CV_THRESH_BINARY,</code>
<li><code>CV_THRESH_BINARY_INV,</code>
</ul>
<dt>blockSize<dd>The size of a pixel neighborhood that is used to calculate a threshold value for the pixel:
3, 5, 7, ...
<dt>param1<dd>The method-dependent parameter.
For the methods <code>CV_ADAPTIVE_THRESH_MEAN_C</code> and <code>CV_ADAPTIVE_THRESH_GAUSSIAN_C</code>
it is a constant subtracted from mean or weighted mean (see the discussion), though it may be negative.
</dl><p>
The function <a href="#decl_cvAdaptiveThreshold">cvAdaptiveThreshold</a> transforms grayscale image to binary image according to
the formulae:</p>
<pre>
thresholdType=<code>CV_THRESH_BINARY</code>:
dst(x,y) = maxValue, if src(x,y)&gt;T(x,y)
           0, otherwise

thresholdType=<code>CV_THRESH_BINARY_INV</code>:
dst(x,y) = 0, if src(x,y)&gt;T(x,y)
           maxValue, otherwise
</pre>
<p>where T<sub>I</sub> is a threshold calculated individually for each pixel.</p>
<p>
For the method <code>CV_ADAPTIVE_THRESH_MEAN_C</code> it is a mean of <code>blockSize</code> &times; <code>blockSize</code>
pixel neighborhood, subtracted by <code>param1</code>.</p><p>
For the method <code>CV_ADAPTIVE_THRESH_GAUSSIAN_C</code> it is a weighted sum (gaussian) of
<code>blockSize</code> &times; <code>blockSize</code> pixel neighborhood, subtracted by <code>param1</code>.</p>


<hr><h3><a name="decl_cvLUT">LUT</a></h3>
<p class="Blurb">Performs look-up table transformation on image</p>
<pre>
CvMat* cvLUT( const CvArr* A, CvArr* B, const CvArr* lut );
</pre><p><dl>
<dt>A<dd>Source array of 8-bit elements.
<dt>B<dd>Destination array of arbitrary depth and of the same number of channels as the
source array.
<dt>lut<dd>Look-up table of 256 elements; should be of the same depth as the
destination array.
</dl><p>
The function <a href="#decl_cvLUT">cvLUT</a> fills the destination array with values of look-up table
entries. Indices of the entries are taken from the source array. That is, the
function processes each pixel as follows:</p>
<pre>
B(x,y)=lut[A(x,y)+&Delta;]
</pre>
where &Delta; is 0 for 8-bit <code>unsigned</code> source image type and 128 for 8-bit <code>signed</code> source image type.
</p>



<hr><h2><a name="ch2_pyramids">Pyramids and the Applications</a></h2>

<hr><h3><a name="decl_cvPyrDown">PyrDown</a></h3>
<p class="Blurb">Downsamples image</p>
<pre>
void cvPyrDown( const CvArr* src, CvArr* dst, int filter=CV_GAUSSIAN_5x5 );
</pre><p><dl>
<dt>src<dd>The source image.
<dt>dst<dd>The destination image, should have 2x smaller width and height than the source.
<dt>filter<dd>Type of the filter used for convolution; only <code>CV_GAUSSIAN_5x5</code> is
currently supported.
</dl><p>
The function <a href="#decl_cvPyrDown">cvPyrDown</a> performs downsampling step of Gaussian pyramid
decomposition. First it convolves source image with the specified filter and
then downsamples the image by rejecting even rows and columns.</p>


<hr><h3><a name="decl_cvPyrUp">PyrUp</a></h3>
<p class="Blurb">Upsamples image</p>
<pre>
void cvPyrUp( const CvArr* src, CvArr* dst, int filter=CV_GAUSSIAN_5x5 );
</pre><p><dl>
<dt>src<dd>The source image.
<dt>dst<dd>The destination image, should have 2x smaller width and height than the source.
<dt>filter<dd>Type of the filter used for convolution; only <code>CV_GAUSSIAN_5x5</code> is
currently supported.
</dl><p>
The function <a href="#decl_cvPyrUp">cvPyrUp</a> performs up-sampling step of Gaussian pyramid decomposition.
First it upsamples the source image by injecting even zero rows and columns and
then convolves result with the specified filter multiplied by 4 for
interpolation. So the destination image is four times larger than the source
image.</p>

<hr><h3><a name="decl_cvPyrSegmentation">PyrSegmentation</a></h3>
<p class="Blurb">Implements image segmentation by pyramids</p>
<pre>
void cvPyrSegmentation( IplImage* src, IplImage* dst,
                        CvMemStorage* storage, CvSeq** comp,
                        int level, double threshold1, double threshold2 );
</pre><p><dl>
<dt>src<dd>The source image.
<dt>dst<dd>The destination image.
<dt>storage<dd>Storage; stores the resulting sequence of connected components.
<dt>comp<dd>Pointer to the output sequence of the segmented components.
<dt>level<dd>Maximum level of the pyramid for the segmentation.
<dt>threshold1<dd>Error threshold for establishing the links.
<dt>threshold2<dd>Error threshold for the segments clustering.
</dl><p>
The function <a href="#decl_cvPyrSegmentation">cvPyrSegmentation</a> implements image segmentation by pyramids. The
pyramid builds up to the level <code>level</code>. The links between any pixel <code>a</code> on level <code>i</code>
and its candidate father pixel <code>b</code> on the adjacent level are established if
<div> <code>p(c(a),c(b))&lt;threshold1</code>.
After the connected components are defined, they are joined into several
clusters. Any two segments A and B belong to the same cluster, if
<div> <code>p(c(A),c(B))&lt;threshold2</code>. The input
image has only one channel, then
<div><code> p(c&sup1;,c&sup2;)=|c&sup1;-c&sup2;|</code>. If the input image has three channels (red,
green and blue), then
<div><code>p(c&sup1;,c&sup2;)=0,3&middot;(c&sup1;<sub>r</sub>-c&sup2;<sub>r</sub>)+0,59&middot;(c&sup1;<sub>g</sub>-c&sup2;<sub>g</sub>)+0,11&middot;(c&sup1;<sub>b</sub>-c&sup2;<sub>b</sub>) </code> .
There may be more than one connected component per a  cluster.
<div>The images <code>src</code> and <code>dst</code> should be 8-bit single-channel or 3-channel images
or equal size</p>


<hr><h2><a name="ch2_ccomp">Connected components</a></h2>

<hr><h3><a name="decl_CvConnectedComp">CvConnectedComp</a></h3>
<p class="Blurb">Connected component</p>
<pre>
    typedef struct CvConnectedComp
    {
        double area; /* area of the segmented component */
        float value; /* gray scale value of the segmented component */
        CvRect rect; /* ROI of the segmented component */
    } CvConnectedComp;
</pre>


<hr><h3><a name="decl_cvFloodFill">FloodFill</a></h3>
<p class="Blurb">Fills a connected component with given color</p>
<pre>
void cvFloodFill( CvArr* img, CvPoint seed, double newVal,
                  double lo=0, double up=0, CvConnectedComp* comp=0,
                  int flags=4, CvArr* mask=0 );
#define CV_FLOODFILL_FIXED_RANGE (1 &lt;&lt; 16)
#define CV_FLOODFILL_MASK_ONLY   (1 &lt;&lt; 17)

</pre><p><dl>
<dt>img<dd>Input image, either 1-,3-channel 8-bit, or single-channel floating-point image.
           It is modified by the function unless CV_FLOODFILL_MASK_ONLY flag is set (see below).
<dt>seed<dd>Coordinates of the seed point inside the image ROI.
<dt>newVal<dd>New value of repainted domain pixels. For 8-bit color images it is a packed color
              (e.g. using <code>CV_RGB</code> macro).
<dt>lo<dd>Maximal lower brightness/color difference between the currently observed pixel and one of its
neighbor belong to the component or seed pixel to add the pixel to component.
In case of 8-bit color images it is packed value.
<dt>up<dd>Maximal upper brightness/color difference between the currently observed pixel and one of its
neighbor belong to the component or seed pixel to add the pixel to component.
In case of 8-bit color images it is packed value.
<dt>comp<dd>Pointer to structure the function fills with the information about the
repainted domain.
<dt>flags<dd>The operation flags. Lower bits contain connectivity value, 4 (by default) or 8,
used within the function. Connectivity determines which neighbors of a pixel are considered.
Upper bits can be 0 or combination of the following flags:<ul>
<li>CV_FLOODFILL_FIXED_RANGE - if set the difference between the current pixel and seed pixel is considered,
                               otherwise difference between neighbor pixels is considered (the range is floating).
<li>CV_FLOODFILL_MASK_ONLY - if set, the function does not fill the image (<code>newVal</code> is ignored),
but the fills mask (that must be non-NULL in this case).
</ul>
<dt>mask<dd>Operation mask, should be singe-channel 8-bit image, 2 pixels wider and 2 pixels taller than
<code>img</code>. If not NULL, the function uses and updates the mask, so user takes responsibility of
initializing <code>mask</code> content. Floodfilling can't go across
non-zero pixels in the mask, for example, an edge detector output can be used as a mask
to stop filling at edges. Or it is possible to use the same mask in multiple calls to the function
to make sure the filled area do not overlap.
</dl><p>
The function <a href="#decl_cvFloodFill">cvFloodFill</a> fills a connected component starting from the seed pixel
where all pixels within the component have close to each other values (prior to filling).
The pixel is considered to belong to the repainted domain if its value <code>I(x,y)</code>
meets the following conditions (the particular cases are specifed after commas):</p>
<pre>
I(x',y')-lo&lt;=I(x,y)&lt;=I(x',y')+up, grayscale image + floating range
I(seed.x,seed.y)-lo&lt;=I(x,y)&lt;=I(seed.x,seed.y)+up, grayscale image + floating range

I(x',y')<sub>r</sub>-lo<sub>r</sub>&lt;=I(x,y)<sub>r</sub>&lt;=I(x',y')<sub>r</sub>+up<sub>r</sub> and
I(x',y')<sub>g</sub>-lo<sub>g</sub>&lt;=I(x,y)<sub>g</sub>&lt;=I(x',y')<sub>g</sub>+up<sub>g</sub> and
I(x',y')<sub>b</sub>-lo<sub>b</sub>&lt;=I(x,y)<sub>b</sub>&lt;=I(x',y')<sub>b</sub>+up<sub>b</sub>, color image + floating range

I(seed.x,seed.y)<sub>r</sub>-lo<sub>r</sub>&lt;=I(x,y)<sub>r</sub>&lt;=I(seed.x,seed.y)<sub>r</sub>+up<sub>r</sub> and
I(seed.x,seed.y)<sub>g</sub>-lo<sub>g</sub>&lt;=I(x,y)<sub>g</sub>&lt;=I(seed.x,seed.y)<sub>g</sub>+up<sub>g</sub> and
I(seed.x,seed.y)<sub>b</sub>-lo<sub>b</sub>&lt;=I(x,y)<sub>b</sub>&lt;=I(seed.x,seed.y)<sub>b</sub>+up<sub>b</sub>, color image + fixed range
</pre>
where <code>I(x',y')</code> is value of one of pixel neighbors (to be added to the connected
component in case of floating range, a pixel should have at least one neigbor with similar brightness)
</p>


<hr><h3><a name="decl_cvFindContours">FindContours</a></h3>
<p class="Blurb">Finds contours in binary image</p>
<pre>
int cvFindContours( CvArr* img, CvMemStorage* storage, CvSeq** firstContour,
                    int headerSize=sizeof(CvContour), CvContourRetrievalMode mode=CV_RETR_LIST,
                    CvChainApproxMethod method=CV_CHAIN_APPROX_SIMPLE );
</pre><p><dl>
<dt>image<dd>The source 8-bit single channel image. Non-zero pixels are treated as
1's, zero pixels remain 0's - that is image treated as <code>binary</code>. To get such a binary image
from grayscale, one may use <a href="#decl_cvThreshold">cvThreshold</a>, <a href="#decl_cvAdaptiveThreshold">cvAdaptiveThreshold</a> or <a href="#decl_cvCanny">cvCanny</a>.
The function modifies the source image content.
<dt>storage<dd>Container of the retrieved contours.
<dt>firstContour<dd>Output parameter, will contain the pointer to the first outer contour.
<dt>headerSize<dd>Size of the sequence header, >=sizeof(<a href="#decl_CvChain">CvChain</a>) if <code>method</code>=CV_CHAIN_CODE,
                  and >=sizeof(CvContour) otherwise.
<dt>mode<dd>Retrieval mode.
</div><ul>
<li><code>CV_RETR_EXTERNAL</code>retrives only the extreme outer contours
<li><code>CV_RETR_LIST</code>retrieves all the contours and puts them in the list
<li><code>CV_RETR_CCOMP</code>retrieves all the contours and organizes them into two-level hierarchy:
                          top level are external boundaries of the components, second level are                          bounda
                          boundaries of the holes
<li><code>CV_RETR_TREE</code>retrieves all the contours and reconstructs the full hierarchy of
                         nested contours
</ul>
<dt>method<dd>Approximation method.
<ul>
<li><code>CV_CHAIN_CODE</code>outputs contours in the Freeman chain code. All other methods output polygons
     (sequences of vertices).
<li><code>CV_CHAIN_APPROX_NONE</code>translates all the points from the chain code into
  points;
<li><code>CV_CHAIN_APPROX_SIMPLE</code>compresses horizontal, vertical, and diagonal segments,
  that is, the function leaves only their ending points;
<li><code>CV_CHAIN_APPROX_TC89_L1,<div>CV_CHAIN_APPROX_TC89_KCOS</code> applies one of the flavors of
  Teh-Chin chain approximation algorithm.
<li><code>CV_LINK_RUNS</code> uses completely different (from the previous methods) algorithm -
linking of horizontal segments of 1's. Only <code>CV_RETR_LIST</code> retrieval mode is allowed by
the method.
</ul>
</dl><p>
The function <a href="#decl_cvFindContours">cvFindContours</a> retrieves contours from the binary image and returns
the number of retrieved contours. The pointer <code>firstContour</code> is filled by the function.
It will contain pointer to the first most outer contour or NULL if no contours is detected (if the image is completely black).
Other contours may be reached from <code>firstContour</code> using <code>h_next</code> and <code>v_next</code> links.
The sample in <a href="#decl_cvDrawContours">cvDrawContours</a> discussion shows how to use contours for connected component
detection. Contours can be also used for shape analysis and object recognition - see <code>squares</code>
sample in CVPR 2001 tutorial course located at SourceForge site.</p>


<hr><h3><a name="decl_cvStartFindContours">StartFindContours</a></h3>
<p class="Blurb">Initializes contour scanning process</p>
<pre>
CvContourScanner cvStartFindContours( IplImage* img, CvMemStorage* storage,
                                      int headerSize, CvContourRetrievalMode mode,
                                      CvChainApproxMethod method );
</pre><p><dl>
<dt>image<dd>The source 8-bit single channel binary image.
<dt>storage<dd>Container of the retrieved contours.
<dt>headerSize<dd>Size of the sequence header, >=sizeof(<a href="#decl_CvChain">CvChain</a>) if <code>method</code>=CV_CHAIN_CODE,
                  and >=sizeof(CvContour) otherwise.
<dt>mode<dd>Retrieval mode, has the same meaning as in <a href="#decl_cvFindContours">cvFindContours</a>.
<dt>method<dd>Approximation method, the same as in <a href="#decl_cvFindContours">cvFindContours</a> except that CV_LINK_RUNS
can not be used here.
</ul>
</dl><p>
The function <a href="#decl_cvStartFindContours">cvStartFindContours</a> initializes and returns pointer to the contour
scanner. The scanner is used further in <a href="#decl_cvFindNextContour">cvFindNextContour</a> to retrieve the rest of contours.
</p>


<hr><h3><a name="decl_cvFindNextContour">FindNextContour</a></h3>
<p class="Blurb">Finds next contour in the image</p>
<pre>
CvSeq* cvFindNextContour( CvContourScanner scanner );
</pre><p><dl>
<dt>scanner<dd>Contour scanner initialized by the function <a href="#decl_cvStartFindContours">cvStartFindContours</a> .
</dl><p>
The function <a href="#decl_cvFindNextContour">cvFindNextContour</a> locates and retrieves the next contour in the image and
returns pointer to it. The function returns NULL, if there is no more contours.</p>


<hr><h3><a name="decl_cvSubstituteContour">SubstituteContour</a></h3>
<p class="Blurb">Replaces retrieved contour</p>
<pre>
void cvSubstituteContour( CvContourScanner scanner, CvSeq* newContour );
</pre><p><dl>
<dt>scanner<dd>Contour scanner initialized by the function cvStartFindContours .
<dt>newContour<dd>Substituting contour.
</dl><p>
The function <a href="#decl_cvSubstituteContour">cvSubstituteContour</a> replaces the retrieved contour, that was returned
from the preceding call of the function <a href="#decl_cvFindNextContour">cvFindNextContour</a> and stored inside
the contour scanner state, with the user-specified contour. The contour is
inserted into the resulting structure, list, two-level hierarchy, or tree,
depending on the retrieval mode. If the parameter <code>newContour</code>=NULL, the retrieved
contour is not included into the resulting structure, nor all of its children
that might be added to this structure later.
</p>


<hr><h3><a name="decl_cvEndFindContours">EndFindContours</a></h3>
<p class="Blurb">Finishes scanning process</p>
<pre>
CvSeq* cvEndFindContours( CvContourScanner* scanner );
</pre><p><dl>
<dt>scanner<dd>Pointer to the contour scanner.
</dl><p>
The function <a href="#decl_cvEndFindContours">cvEndFindContours</a> finishes the scanning process and returns the
pointer to the first contour on the highest level.</p>


<hr><h3><a name="decl_cvDrawContours">DrawContours</a></h3>
<p class="Blurb">Draws contour outlines or interiors in the image</p>
<pre>
void cvDrawContours( CvArr *image, CvSeq* contour,
                     double external_color, double hole_color,
                     int max_level, int thickness=1,
                     int connectivity=8 );
</pre><p><dl>
<dt>image<dd>Image where the contours are to be drawn. Like in any other drawing
function, the contours are clipped with the ROI.
<dt>contour<dd>Pointer to the first contour.
<dt>externalColor<dd>Color to draw external contours with.
<dt>holeColor<dd>Color to draw holes with.
<dt>maxLevel<dd>Maximal level for drawn contours. If 0, only <code>contour</code> is drawn. If
1, the contour and all contours after it on the same level are drawn. If 2, all
contours after and all contours one level below the contours are drawn, etc.
If the value is negative, the function does not draw the contours following after <code>contour</code>
but draws child contours of <code>contour</code> up to abs(<code>maxLevel</code>)-1 level.
<dt>thickness<dd>Thickness of lines the contours are drawn with. If it is negative (e.g. =CV_FILLED),
the contour interiors are drawn.
<dt>connectivity<dd>Connectivity of line segments of the contour outlines.
</dl><p>
The function <a href="#decl_cvDrawContours">cvDrawContours</a> draws contour outlines in the image if <code>thickness</code>&gt;=0
or fills area bounded by the contours if <code>thickness</code>&lt;0.
</p>
<h4>Example. Connected component detection via contour functions</h4>
<pre>
#include "cv.h"
#include "highgui.h"

int main( int argc, char** argv )
{
    IplImage* src;
    // the first command line parameter must be file name of binary (black-n-white) image
    if( argc == 2 && (src=cvLoadImage(argv[1], 0))!= 0)
    {
        IplImage* dst = cvCreateImage( cvGetSize(src), 8, 3 );
        CvMemStorage* storage = cvCreateMemStorage(0);
        CvSeq* contour = 0;

        cvThreshold( src, src, 1, 255, CV_THRESH_BINARY );
        cvNamedWindow( "Source", 1 );
        cvShowImage( "Source", src );

        cvFindContours( src, storage, &contour, sizeof(CvContour), CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE );
        cvZero( dst );

        for( ; contour != 0; contour = contour->h_next )
        {
            int color = CV_RGB( rand(), rand(), rand() );
            /* replace CV_FILLED with 1 to see the outlines */
            cvDrawContours( dst, contour, color, color, -1, CV_FILLED, 8 );
        }

        cvNamedWindow( "Components", 1 );
        cvShowImage( "Components", dst );
        cvWaitKey(0);
    }
}
</pre>
<p>Replace CV_FILLED with 1 in the sample below to see the contour outlines
</p>


<hr><h2><a name="ch2_moments">Image and contour moments</a></h2>

<hr><h3><a name="decl_cvMoments">Moments</a></h3>
<p class="Blurb">Calculates all moments up to third order of a polygon or rasterized shape</p>
<pre>
void cvMoments( const CvArr* arr, CvMoments* moments, int isBinary=0 );
</pre><p><dl>
<dt>arr<dd>Image (1-channel or 3-channel with COI set) or polygon (CvSeq of points of a vector of
points).
<dt>moments<dd>Pointer to returned moment state structure.
<dt>isBinary<dd>(For images only) If the flag is non-zero, all the zero pixel values are treated as
zeroes, all the others are treated as ones.
</dl><p>
The function <a href="#decl_cvMoments">cvMoments</a> calculates spatial and central moments up to the third order and
writes them to <code>moments</code>. The moments may be used then to calculate gravity center of the shape,
its area, main axises and various shape characeteristics including 7 Hu invariants.</p>


<hr><h3><a name="decl_cvGetSpatialMoment">GetSpatialMoment</a></h3>
<p class="Blurb">Retrieves spatial moment from moment state structure</p>
<pre>
double cvGetSpatialMoment( CvMoments* moments, int j, int i );
</pre><p><dl>
<dt>moments<dd>The moment state, calculated by <a href="#decl_cvMoments">cvMoments</a>.
<dt>j<dd>x-order of the retrieved moment, j &gt;= 0.
<dt>i<dd>y-order of the retrieved moment, i &gt;= 0 and i + j &lt;= 3.
</dl><p>
The function <a href="#decl_cvGetSpatialMoment">cvGetSpatialMoment</a> retrieves the spatial moment, which in case of
image moments is defined as:</p>
<pre>
M<sub>ji</sub>=sum<sub>x,y</sub>(I(x,y)&bull;x<sup>j</sup>&bull;y<sup>i</sup>)
</pre>
<p>where <code>I(x,y)</code> is the intensity of the pixel <code>(x, y)</code>.


<hr><h3><a name="decl_cvGetCentralMoment">GetCentralMoment</a></h3>
<p class="Blurb">Retrieves central moment from moment state structure</p>
<pre>
double cvGetCentralMoment( CvMoments* moments, int j, int i );
</pre><p><dl>
<dt>moments<dd>Pointer to the moment state structure.
<dt>j<dd>x-order of the retrieved moment, j &gt;= 0.
<dt>i<dd>y-order of the retrieved moment, i &gt;= 0 and i + j &lt;= 3.
</dl><p>
The function<a href="#decl_cvGetCentralMoment">cvGetCentralMoment</a> retrieves the central moment, which in case of
image moments is defined as:</p>
<pre>
&mu;<sub>ij</sub>=sum<sub>x,y</sub>(I(x,y)&bull;(x-x<sub>c</sub>)<sup>j</sup>&bull;(y-y<sub>c</sub>)<sup>i</sup>),

where x<sub>c</sub>=M<sub>10</sub>/M<sub>00</sub>, y<sub>c</sub>=M<sub>01</sub>/M<sub>00</sub> - coordinates of the gravity center
</pre>


<hr><h3><a name="decl_cvGetNormalizedCentralMoment">GetNormalizedCentralMoment</a></h3>
<p class="Blurb">Retrieves normalized central moment from moment state structure</p>
<pre>
double cvGetNormalizedCentralMoment( CvMoments* moments, int x_order, int y_order );
</pre><p><dl>
<dt>moments<dd>Pointer to the moment state structure.
<dt>j<dd>x-order of the retrieved moment, j &gt;= 0.
<dt>i<dd>y-order of the retrieved moment, i &gt;= 0 and i + j &lt;= 3.
</dl><p>
The function <a href="#decl_cvGetNormalizedCentralMoment">cvGetNormalizedCentralMoment</a> retrieves the normalized central moment, which in case of
image moments is defined as:</p>
<pre>
&eta;<sub>ij</sub>= &mu;<sub>ij</sub>/M<sub>00</sub><sup>((i+j)/2+1)</sup>
</pre>


<hr><h3><a name="decl_cvGetHuMoments">GetHuMoments</a></h3>
<p class="Blurb">Calculates seven Hu invariants</p>
<pre>
void cvGetHuMoments( CvMoments* moments, CvHuMoments* HuMoments );
</pre><p><dl>
<dt>moments<dd>Pointer to the moment state structure.
<dt>HuMoments<dd>Pointer to Hu moments structure.
</dl><p>
The function <a href="#decl_cvGetHuMoments">cvGetHuMoments</a> calculates seven Hu invariants that are defined as:
<pre>
 h<sub>1</sub>=&eta;<sub>20</sub>+&eta;<sub>02</sub>

 h<sub>2</sub>=(&eta;<sub>20</sub>-&eta;<sub>02</sub>)&sup2;+4&eta;<sub>11</sub>&sup2;

 h<sub>3</sub>=(&eta;<sub>30</sub>-3&eta;<sub>12</sub>)&sup2;+ (3&eta;<sub>21</sub>-&eta;<sub>03</sub>)&sup2;

 h<sub>4</sub>=(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;+ (&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;

 h<sub>5</sub>=(&eta;<sub>30</sub>-3&eta;<sub>12</sub>)(&eta;<sub>30</sub>+&eta;<sub>12</sub>)[(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;-3(&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;]+(3&eta;<sub>21</sub>-&eta;<sub>03</sub>)(&eta;<sub>21</sub>+&eta;<sub>03</sub>)[3(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;-(&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;]

 h<sub>6</sub>=(&eta;<sub>20</sub>-&eta;<sub>02</sub>)[(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;- (&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;]+4&eta;<sub>11</sub>(&eta;<sub>30</sub>+&eta;<sub>12</sub>)(&eta;<sub>21</sub>+&eta;<sub>03</sub>)

 h<sub>7</sub>=(3&eta;<sub>21</sub>-&eta;<sub>03</sub>)(&eta;<sub>21</sub>+&eta;<sub>03</sub>)[3(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;-(&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;]-(&eta;<sub>30</sub>-3&eta;<sub>12</sub>)(&eta;<sub>21</sub>+&eta;<sub>03</sub>)[3(&eta;<sub>30</sub>+&eta;<sub>12</sub>)&sup2;-(&eta;<sub>21</sub>+&eta;<sub>03</sub>)&sup2;]
</pre>
<p>
These values are proved to be invariants to the image scale, rotation, and
reflection except the seventh one, whose sign is changed by reflection.
</p>


<hr><h2><a name="ch2_transforms">Special Image Transforms</a></h2>

<hr><h3><a name="decl_cvHoughLines">HoughLines</a></h3>
<p class="Blurb">Finds lines in binary image using Hough transform</p>
<pre>
CvSeq* cvHoughLines2( CvArr* image, void* lineStorage, int method,
                      double dRho, double dTheta, int threshold,
                      double param1=0, double param2 );
</pre><p><dl>
<dt>image<dd>Source 8-bit single-channel (binary) image. It may be modified by the function.
<dt>lineStorage<dd>The storage for the lines detected. It can be a memory storage (in this case
a sequence of lines is created in the storage and returned by the function) or single row/single column
matrix (CvMat*) of a particular type (see below) where the lines' parameters are written.
The matrix header is modified by the function so its <code>cols</code>/<code>rows</code> contains
a number of lines detected (that is a matrix is truncated to fit exactly the detected lines,
though no data is deallocated - only the header is modified). In the latter case
if the actual number of lines exceeds the matrix size, the maximum possible number of lines is returned
(the lines are not sorted by length, confidence or whatever criteria).
<dt>method<dd>The Hough transform variant, one of:<ul>
    <li><code>CV_HOUGH_STANDARD</code> - classical or standard Hough transform. Every line is represented by two floating-point numbers
         (&rho;, &theta;), where &rho; is a distance between (0,0) point and the line, and &theta; is the angle
         between x-axis and the normal to the line. Thus, the matrix must be (the created sequence will
         be) of CV_32FC2 type.
    <li><code>CV_HOUGH_PROBABILISTIC</code> - probabilistic Hough transform (more efficient in case if picture contains
    a few long linear segments). It returns line segments rather than the whole lines.
    Every segment is represented by starting and ending points, and the matrix must be
    (the created sequence will be) of CV_32SC4 type.
    <li><code>CV_HOUGH_MULTI_SCALE</code> - multi-scale variant of classical Hough transform.
    The lines are encoded the same way as in CV_HOUGH_CLASSICAL.
    </ul>
<dt>dRho<dd>Distance resolution in pixel-related units.
<dt>dTheta<dd>Angle resolution measured in radians.
<dt>threshold<dd>Threshold parameter. A line is returned by the function if the corresponding
accumulator value is greater than <code>threshold</code>.
<dt>param1<dd>The first method-dependent parameter:<ul>
              <li>For classical Hough transform it is not used (0).
              <li>For probabilistic Hough transform it is the minimum line length.
              <li>For multi-scale Hough transform it is divisor for distance resolution <code>dRho</code>.
              (The coarse distance resolution will be <code>dRho</code> and the accurate resolution will be (<code>dRho</code> / <code>param1</code>)).
              </ul>
<dt>param2<dd>The second method-dependent parameter:<ul>
              <li>For classical Hough transform it is not used (0).
              <li>For probabilistic Hough transform it is the maximum gap between line segments lieing on the same line to
                  treat them as the single line segment (i.e. to join them).
              <li>For multi-scale Hough transform it is divisor for angle resolution <code>dTheta</code>.
              (The coarse angle resolution will be <code>dTheta</code> and the accurate resolution will be (<code>dTheta</code> / <code>param2</code>)).
              </ul>
</dl><p>
The function <a href="#decl_cvHoughLines2">cvHoughLines2</a> implements a few variants of Hough transform for line detection.</p>
<h4>Example. Detecting lines with Hough transform.</h4>
<pre>
/* This is a standalone program. Pass an image name as a first parameter of the program.
   Switch between standard and probabilistic Hough transform by changing "#if 1" to "#if 0" and back */
#include &lt;cv.h&gt;
#include &lt;highgui.h&gt;
#include &lt;math.h&gt;

int main(int argc, char** argv)
{
    IplImage* src;
    if( argc == 2 && (src=cvLoadImage(argv[1], 0))!= 0)
    {
        IplImage* dst = cvCreateImage( cvGetSize(src), 8, 1 );
        IplImage* color_dst = cvCreateImage( cvGetSize(src), 8, 3 );
        CvMemStorage* storage = cvCreateMemStorage(0);
        CvSeq* lines = 0;
        int i;
        cvCanny( src, dst, 50, 200, 3 );
        cvCvtColor( dst, color_dst, CV_GRAY2BGR );
#if 1
        lines = cvHoughLines2( dst, storage, CV_HOUGH_CLASSICAL, 1, CV_PI/180, 150, 0, 0 );

        for( i = 0; i &lt; lines->total; i++ )
        {
            float* line = (float*)cvGetSeqElem(lines,i);
            float rho = line[0];
            float theta = line[1];
            CvPoint pt1, pt2;
            double a = cos(theta), b = sin(theta);
            if( fabs(a) &lt; 0.001 )
            {
                pt1.x = pt2.x = cvRound(rho);
                pt1.y = 0;
                pt2.y = color_dst->height;
            }
            else if( fabs(b) &lt; 0.001 )
            {
                pt1.y = pt2.y = cvRound(rho);
                pt1.x = 0;
                pt2.x = color_dst->width;
            }
            else
            {
                pt1.x = 0;
                pt1.y = cvRound(rho/b);
                pt2.x = cvRound(rho/a);
                pt2.y = 0;
            }
            cvLine( color_dst, pt1, pt2, CV_RGB(255,0,0), 3, 8 );
        }
#else
        lines = cvHoughLines2( dst, storage, CV_HOUGH_PROBABILISTIC, 1, CV_PI/180, 80, 30, 10 );
        for( i = 0; i &lt; lines-&gt;total; i++ )
        {
            CvPoint* line = (CvPoint*)cvGetSeqElem(lines,i);
            cvLine( color_dst, line[0], line[1], CV_RGB(255,0,0), 3, 8 );
        }
#endif
        cvNamedWindow( "Source", 1 );
        cvShowImage( "Source", src );

        cvNamedWindow( "Hough", 1 );
        cvShowImage( "Hough", color_dst );

        cvWaitKey(0);
    }
}
</pre>
<p>This is the sample picture the function parameters have been tuned for:</p>
<p>
<img src="pics/building.jpg" width=320 height=240></a>
</p>
<p>
And this is the output of the above program in case of probabilistic Hough transform ("#if 0" case):
</p>
<p>
<img src="pics/houghp.png" width=320 height=240></a>
</p>


<hr><h3><a name="decl_cvDistTransform">DistTransform</a></h3>
<p class="Blurb">Calculates distance to closest zero pixel for all non-zero pixels of source
image</p>
<pre>
void cvDistTransform( const CvArr* src, CvArr* dst, CvDisType disType=CV_DIST_L2,
                      int maskSize=3, float* mask=0 );
</pre><p><dl>
<dt>src<dd>Source 8-bit single-channel (binary) image.
<dt>dst<dd>Output image with calculated distances (32-bit floating-point, single-channel).
<dt>disType<dd>Type of distance; can be <code>CV_DIST_L1, CV_DIST_L2, CV_DIST_C</code> or
<code>CV_DIST_USER</code>.
<dt>maskSize<dd>Size of distance transform mask; can be 3 or 5. In case if <code>CV_DIST_L1</code> or
<code>CV_DIST_C</code> the parameter is forced to 3, because 5&times;5 mask gives the same result
as 3&times;3 in this case yet it is slower.
<dt>mask<dd>User-defined mask in case of user-defined distance, it consists of 2 numbers
(horizontal/vertical shift cost, diagonal shift cost) in case of 3&times;3 mask and
3 numbers (horizontal/vertical shift cost, diagonal shift cost, knight's move cost)
in case of 5&times;5 mask.
</dl><p>
The function <a href="#decl_cvDistTransform">cvDistTransform</a> calculates the approximated distance from every binary image pixel
to the nearest zero pixel. For zero pixels the function sets the zero distance, for others it finds
the shortest path consisting of basic shifts: horizontal, vertical, diagonal or knight's move (the
latest is available for 5&times;5 mask). The overal distance is calculated as a sum of these basic distances.
Because the distance function should be symmetric, all the horizontal and vertical shifts must have
the same cost (that is denoted as <code>a</code>), all the diagonal shifts must have the same cost
(denoted <code>b</code>), and all knight's moves' must have the same cost (denoted <code>c</code>).
For <code>CV_DIST_C</code> and <code>CV_DIST_L1</code> types the distance is calculated precisely,
whereas for <code>CV_DIST_L2</code> (Euclidian distance) the distance can be calculated only with
some relative error (5&times;5 mask gives more accurate results), OpenCV uses the values suggested in
<a href="#disttrans_paper">[Borgefors86]</a>:</p>
<pre>
CV_DIST_C (3&times;3):
a=1, b=1

CV_DIST_L1 (3&times;3):
a=1, b=2

CV_DIST_L2 (3&times;3):
a=0.955, b=1.3693

CV_DIST_L2 (5&times;5):
a=1, b=1.4, c=2.1969
</pre>
<p>
And below are samples of distance field (black (0) pixel is in the middle of white square)
in case of user-defined distance:
</p>
<h4>User-defined 3&times;3 mask (a=1, b=1.5)</h4>
<p>
   <table border=3 cellpadding=5>
   <tr><td>4.5</td><td>4</td><td>3.5</td><td>3</td><td>3.5</td><td>4</td><td>4.5</td></tr>
   <tr><td>4</td><td>3</td><td>2.5</td><td>2</td><td>2.5</td><td>3</td><td>4</td></tr>
   <tr><td>3.5</td><td>2.5</td><td>1.5</td><td>1</td><td>1.5</td><td>2.5</td><td>3.5</td></tr>
   <tr><td>3</td><td>2</td><td>1</td><td>0</td><td>1</td><td>2</td><td>3</td></tr>
   <tr><td>3.5</td><td>2.5</td><td>1.5</td><td>1</td><td>1.5</td><td>2.5</td><td>3.5</td></tr>
   <tr><td>4</td><td>3</td><td>2.5</td><td>2</td><td>2.5</td><td>3</td><td>4</td></tr>
   <tr><td>4.5</td><td>4</td><td>3.5</td><td>3</td><td>3.5</td><td>4</td><td>4.5</td></tr>
   </table>
</p>
<h4>User-defined 5&times;5 mask (a=1, b=1.5, c=2)</h4>
<p>
   <table border=3 cellpadding=5>
   <tr><td>4.5</td><td>3.5</td><td>3</td><td>3</td><td>3</td><td>3.5</td><td>4.5</td></tr>
   <tr><td>3.5</td><td>3</td><td>2</td><td>2</td><td>2</td><td>3</td><td>3.5</td></tr>
   <tr><td>3</td><td>2</td><td>1.5</td><td>1</td><td>1.5</td><td>2</td><td>3</td></tr>
   <tr><td>3</td><td>2</td><td>1</td><td>0</td><td>1</td><td>2</td><td>3</td></tr>
   <tr><td>3</td><td>2</td><td>1.5</td><td>1</td><td>1.5</td><td>2</td><td>3</td></tr>
   <tr><td>3.5</td><td>3</td><td>2</td><td>2</td><td>2</td><td>3</td><td>3.5</td></tr>
   <tr><td>4</td><td>3.5</td><td>3</td><td>3</td><td>3</td><td>3.5</td><td>4</td></tr>
   </table>
</p>

<p>Typically, for fast coarse distance estimation CV_DIST_L2, 3&times;3 mask is used,
and for more accurate distance estimation CV_DIST_L2, 5&times;5 mask is used.</p>

<p><a name="disttrans_paper"></a><b>
[Borgefors86] Gunilla Borgefors, "Distance Transformations in Digital Images". Computer Vision, Graphics and Image Processing 34, 344-371 (1986).
</b></p>


<hr><h2><a name="ch2_histograms">Histogram Functions</a></h2>

<hr><h3><a name="decl_CvHistogram ">CvHistogram</a></h3>
<p class="Blurb">Muti-dimensional histogram</p>
<pre>
    typedef struct CvHistogram
    {
        int header_size; /* header's size */
        CvHistType type; /* type of histogram */
        int flags; /* histogram's flags */
        int c_dims; /* histogram's dimension */
        int dims[CV_HIST_MAX_DIM]; /* every dimension size */
        int mdims[CV_HIST_MAX_DIM]; /* coefficients for fast access to element */
        /* &m[a,b,c] = m + a*mdims[0] + b*mdims[1] + c*mdims[2] */
        float* thresh[CV_HIST_MAX_DIM]; /* bin boundaries arrays for every dimension */
        float* array; /* all the histogram data, expanded into the single row */
    struct CvNode* root; /* root of balanced tree storing histogram bins */
        CvSet* set; /* pointer to memory storage (for the balanced tree) */
        int* chdims[CV_HIST_MAX_DIM]; /* cache data for fast calculating */
    } CvHistogram;
</pre>

<hr><h3><a name="decl_cvCreateHist">CreateHist</a></h3>
<p class="Blurb">Creates histogram</p>
<pre>
CvHistogram* cvCreateHist( int cDims, int* dims, int type,
                           float** ranges=0, int uniform=1 );
</pre><p><dl>
<dt>cDims<dd>Number of histogram dimensions.
<dt>dims<dd>Array of histogram dimension sizes.
<dt>type<dd>Histogram representation format: <code>CV_HIST_ARRAY</code> means that histogram data is
represented as an multi-dimensional dense array <a href="#decl_CvMatND">CvMatND</a>;
<code>CV_HIST_TREE</code> means that histogram data is represented
as a multi-dimensional sparse array <a href="#decl_CvSparseMat">CvSparseMat</a>.
<dt>ranges<dd>Array of ranges for histogram bins. Its meaning depends on the <code>uniform</code> parameter value.
The ranges are used for when histogram is calculated or backprojected to determine, which histogram bin
corresponds to which value/tuple of values from the input image[s].
<dt>uniform<dd>Uniformity flag; if not 0, the histogram has evenly spaced bins and
for every <code>0&lt;=i&lt;cDims</code> <code>ranges[i]</code> is array of two numbers: lower and upper
boundaries for the i-th histogram dimension. The whole range [lower,upper] is split then
into <code>dims[i]</code> equal parts to determine <code>i-th</code> input tuple value ranges for every histogram bin.
And if <code>uniform=0</code>, then <code>i-th</code> element of <code>ranges</code> array contains <code>dims[i]+1</code> elements:
<code>lower<sub>0</sub>, upper<sub>0</sub>, lower<sub>1</sub>, upper<sub>1</sub> == lower<sub>2</sub>, ..., upper<sub>dims[i]-1</sub></code>,
where <code>lower<sub>j</sub></code> and <code>upper<sub>j</sub></code> are lower and upper
boundaries of <code>i-th</code> input tuple value for <code>j-th</code> bin, respectively.
In either case, the input values that are beyond the specified range for a histogram bin, are not
counted by <a href="#decl_cvCalcHist">cvCalcHist</a> and filled with 0 by <a href="#decl_cvCalcBackProject">cvCalcBackProject</a>.
</dl><p>
The function <a href="#decl_cvCreateHist">cvCreateHist</a> creates a histogram of the specified size and returns
the pointer to the created histogram. If the array <code>ranges</code> is 0, the histogram
bin ranges must be specified later via the function <a href="#decl_cvSetHistBinRanges">cvSetHistBinRanges</a>, though
<a href="#decl_cvCalcHist">cvCalcHist</a> and <a href="#decl_cvCalcBackProject">cvCalcBackProject</a> may process 8-bit images without setting
bin ranges, they assume equally spaced in 0..255 bins.</p>


<hr><h3><a name="decl_cvSetHistBinRanges">SetHistBinRanges</a></h3>
<p class="Blurb">Sets bounds of histogram bins</p>
<pre>
void cvSetHistBinRanges( CvHistogram* hist, float** ranges, int uniform=1 );
</pre><p><dl>
<dt>hist<dd>Histogram.
<dt>ranges<dd>Array of bin ranges arrays, see <a href="#decl_cvCreateHist">cvCreateHist</a>.
<dt>uniform<dd>Uniformity flag, see <a href="#decl_cvCreateHist">cvCreateHist</a>.
</dl><p>
The function <a href="#decl_cvSetHistBinRanges">cvSetHistBinRanges</a> is a stand-alone function for setting bin ranges
in the histogram. For more detailed description of the parameters <code>ranges</code> and
<code>uniform</code> see <a href="#decl_cvCalcHist">cvCalcHist</a> function,
that can initialize the ranges as well.
Ranges for histogram bins must be set before the histogram is calculated or
backproject of the histogram is calculated.
</p>


<hr><h3><a name="decl_cvReleaseHist">ReleaseHist</a></h3>
<p class="Blurb">Releases histogram</p>
<pre>
void cvReleaseHist( CvHistogram** hist );
</pre><p><dl>
<dt>hist<dd>Double pointer to the released histogram.
</dl><p>
The function <a href="#decl_cvReleaseHist">cvReleaseHist</a> releases the histogram (header and the data).
The pointer to histogram is cleared by the function. If <code>*hist</code> pointer is already
<code>NULL</code>, the function does nothing.</p>


<hr><h3><a name="decl_cvClearHist">ClearHist</a></h3>
<p class="Blurb">Clears histogram</p>
<pre>
void cvClearHist( CvHistogram* hist );
</pre><p><dl>
<dt>hist<dd>Histogram.
</dl><p>
The function <a href="#decl_cvClearHist">cvClearHist</a> sets all histogram bins to 0 in case of dense histogram and
removes all histogram bins in case of sparse array.</p>


<hr><h3><a name="decl_cvMakeHistHeaderForArray">MakeHistHeaderForArray</a></h3>
<p class="Blurb">Makes a histogram out of array</p>
<pre>
void cvMakeHistHeaderForArray( int cDims, int* dims, CvHistogram* hist,
                               float* data, float** ranges=0, int uniform=1 );
</pre><p><dl>
<dt>cDims<dd>Number of histogram dimensions.
<dt>dims<dd>Array of histogram dimension sizes.
<dt>hist<dd>The histogram header initialized by the function.
<dt>data<dd>Array that will be used to store histogram bins.
<dt>ranges<dd>Histogram bin ranges, see <a href="#decl_cvCreateHist">cvCreateHist</a>.
<dt>uniform<dd>Uniformity flag, see <a href="#decl_cvCreateHist">cvCreateHist</a>.
</dl><p>
The function <a href="#decl_cvMakeHistHeaderForArray">cvMakeHistHeaderForArray</a> initializes the histogram, which header and
bins are allocated by user. No <a href="#decl_cvReleaseHist">cvReleaseHist</a> need to be called afterwards.
The histogram will be dense, sparse histogram can not be initialized this way.
</p>


<hr><h3><a name="decl_cvQueryHistValue_1D">QueryHistValue_1D</a></h3>
<p class="Blurb">Queries value of histogram bin</p>
<pre>
#define cvQueryHistValue_1D( hist, idx0 ) \
    cvGetReal1D( (hist)->bins, (idx0) )
#define cvQueryHistValue_2D( hist, idx0, idx1 ) \
    cvGetReal2D( (hist)->bins, (idx0), (idx1) )
#define cvQueryHistValue_3D( hist, idx0, idx1, idx2 ) \
    cvGetReal3D( (hist)->bins, (idx0), (idx1), (idx2) )
#define cvQueryHistValue_nD( hist, idx ) \
    cvGetRealND( (hist)->bins, (idx) )
</pre><p><dl>
<dt>hist<dd>Histogram.
<dt>idx0, idx1, idx2, idx3<dd>Indices of the bin.
<dt>idx<dd>Array of indices
</dl><p>
The macros <a href="#decl_cvQueryHistValue_*D">cvQueryHistValue_*D</a> return the value of the specified bin of 1D, 2D, 3D or
nD histogram. In case of sparse histogram the function returns 0, if the bin is not present in the
histogram, and no new bin is created.
</p>


<hr><h3><a name="decl_cvGetHistValue_1D">GetHistValue_1D</a></h3>
<p class="Blurb">Returns pointer to histogram bin</p>
<pre>
#define cvGetHistValue_1D( hist, idx0 ) \
    ((float*)(cvPtr1D( (hist)->bins, (idx0), 0 ))
#define cvGetHistValue_2D( hist, idx0, idx1 ) \
    ((float*)(cvPtr2D( (hist)->bins, (idx0), (idx1), 0 ))
#define cvGetHistValue_3D( hist, idx0, idx1, idx2 ) \
    ((float*)(cvPtr3D( (hist)->bins, (idx0), (idx1), (idx2), 0 ))
#define cvGetHistValue_nD( hist, idx ) \
    ((float*)(cvPtrND( (hist)->bins, (idx), 0 ))
</pre><p><dl>
<dt>hist<dd>Histogram.
<dt>idx0, idx1, idx2, idx3<dd>Indices of the bin.
<dt>idx<dd>Array of indices
</dl><p>
The macros <a href="#decl_cvGetHistValue_*D">cvGetHistValue_*D</a> return pointer to the specified bin of 1D, 2D, 3D or
nD histogram. In case of sparse histogram the function creates a new bins and fills it with 0,
if it does not exists.
</p>


<hr><h3><a name="decl_cvGetMinMaxHistValue">GetMinMaxHistValue</a></h3>
<p class="Blurb">Finds minimum and maximum histogram bins</p>
<pre>
void cvGetMinMaxHistValue( const CvHistogram* hist,
                           float* minVal, float* maxVal,
                           int* minIdx =0, int* maxIdx =0);
</pre><p><dl>
<dt>hist<dd>Histogram.
<dt>minVal<dd>Pointer to the minimum value of the histogram; can be NULL.
<dt>maxVal<dd>Pointer to the maximum value of the histogram; can be NULL.
<dt>minIdx<dd>Pointer to the array of coordinates for minimum. If not NULL, must have
<code>hist->c_dims</code> elements to store the coordinates.
<dt>maxIdx<dd>Pointer to the array of coordinates for maximum. If not NULL, must have
<code>hist->c_dims</code> elements to store the coordinates.
</dl><p>
The function <a href="#decl_cvGetMinMaxHistValue">cvGetMinMaxHistValue</a> finds the minimum and maximum histogram bins and
their positions. In case of several maximums or minimums the earliest in lexicographical order
extrema locations are returned.</p>


<hr><h3><a name="decl_cvNormalizeHist">NormalizeHist</a></h3>
<p class="Blurb">Normalizes histogram</p>
<pre>
void cvNormalizeHist( CvHistogram* hist, double factor );
</pre><p><dl>
<dt>hist<dd>Pointer to the histogram.
<dt>factor<dd>Normalization factor.
</dl><p>
The function <a href="#decl_cvNormalizeHist">cvNormalizeHist</a> normalizes the histogram bins by scaling them,
such that the sum of the bins becomes equal to <code>factor</code>.</p>


<hr><h3><a name="decl_cvThreshHist">ThreshHist</a></h3>
<p class="Blurb">Thresholds histogram</p>
<pre>
void cvThreshHist( CvHistogram* hist, double thresh );
</pre><p><dl>
<dt>hist<dd>Pointer to the histogram.
<dt>thresh<dd>Threshold level.
</dl><p>
The function <a href="#decl_cvThreshHist">cvThreshHist</a> clears histogram bins that are below the specified
level.</p>


<hr><h3><a name="decl_cvCompareHist">CompareHist</a></h3>
<p class="Blurb">Compares two dense histograms</p>
<pre>
double cvCompareHist( const CvHistogram* H1, const CvHistogram* H2,
                      CvCompareMethod method );
</pre><p><dl>
<dt>H1<dd>The first dense histogram.
<dt>H2<dd>The second dense histogram.
<dt>method<dd>Comparison method, one of:
<ul>
<li>CV_COMP_CORREL;
<li>CV_COMP_CHISQR;
<li>CV_COMP_INTERSECT.
</ul>
</dl><p>
The function <a href="#decl_cvCompareHist">cvCompareHist</a> compares two histograms using specified method and
returns the comparison result. It processes as following:</p>
<pre>
Correlation (method=CV_COMP_CORREL):
d(H<sub>1</sub>,H<sub>2</sub>)=sum<sub>I</sub>(H'<sub>1</sub>(I)&bull;H'<sub>2</sub>(I))/sqrt(sum<sub>I</sub>[H'<sub>1</sub>(I)<sup>2</sup>]&bull;sum<sub>I</sub>[H'<sub>2</sub>(I)<sup>2</sup>])
where
H'<sub>k</sub>(I)=H<sub>k</sub>(I)-1/N&bull;sum<sub>J</sub>H<sub>k</sub>(J) (N=number of histogram bins)

Chi-Square (method=CV_COMP_CHISQR):
d(H<sub>1</sub>,H<sub>2</sub>)=sum<sub>I</sub>[(H<sub>1</sub>(I)-H<sub>2</sub>(I))/(H<sub>1</sub>(I)+H<sub>2</sub>(I))]

Intersection (method=CV_COMP_INTERSECT):
d(H<sub>1</sub>,H<sub>2</sub>)=sum<sub>I</sub>max(H<sub>1</sub>(I),H<sub>2</sub>(I))
</pre>
<p>Note, that the function can operate on dense histogram only. To compare sparse histogram or more
general sparse configurations of weighted points, consider <a href="#decl_cvCalcEMD">cvCalcEMD</a> function.</p>


<hr><h3><a name="decl_cvCopyHist">CopyHist</a></h3>
<p class="Blurb">Copies histogram</p>
<pre>
void cvCopyHist( CvHistogram* src, CvHistogram** dst );
</pre><p><dl>
<dt>src<dd>Source histogram.
<dt>dst<dd>Pointer to destination histogram.
</dl><p>
The function <a href="#decl_cvCopyHist">cvCopyHist</a> makes a copy of the histogram. If the second histogram
pointer <code>*dst</code> is NULL, a new histogram of the same size as <code>src</code> is created.
Otherwise, both histograms must have equal types and sizes.
Then the function copies the source histogram bins values to destination histogram and
sets the same as <code>src</code>'s value ranges.</p>


<hr><h3><a name="decl_cvCalcHist">CalcHist</a></h3>
<p class="Blurb">Calculates histogram of image(s)</p>
<pre>
void cvCalcHist( IplImage** img, CvHistogram* hist,
                 int doNotClear=0, const CvArr* mask=0 );
</pre><p><dl>
<dt>img<dd>Source images (though, you may pass CvMat** as well).
<dt>hist<dd>Pointer to the histogram.
<dt>doNotClear<dd>Clear flag, if it is non-zero, the histogram is not cleared before calculation.
It may be useful for iterative histogram update.
<dt>mask<dd>The operation mask, determines what pixels of the source images are counted.
</dl><p>
The function <a href="#decl_cvCalcHist">cvCalcHist</a> calculates the histogram of one or more single-channel images.
The elements of a tuple that is used to increment a histogram bin are taken at the same
location from the corresponding input images.</p>
<h4>Sample. Calculating and displaying 2D Hue-Saturation histogram of a color image</h4>
<pre>
#include &lt;cv.h&gt;
#include &lt;highgui.h&gt;

int main( int argc, char** argv )
{
    IplImage* src;
    if( argc == 2 && (src=cvLoadImage(argv[1], 1))!= 0)
    {
        IplImage* h_plane = cvCreateImage( cvGetSize(src), 8, 1 );
        IplImage* s_plane = cvCreateImage( cvGetSize(src), 8, 1 );
        IplImage* v_plane = cvCreateImage( cvGetSize(src), 8, 1 );
        IplImage* planes[] = { h_plane, s_plane };
        IplImage* hsv = cvCreateImage( cvGetSize(src), 8, 3 );
        int h_bins = 30, s_bins = 32;
        int hist_size[] = {h_bins, s_bins};
        float h_ranges[] = { 0, 180 }; /* hue varies from 0 (~0&deg;red) to 180 (~360&deg;red again) */
        float s_ranges[] = { 0, 255 }; /* saturation varies from 0 (black-gray-white) to 255 (pure spectrum color) */
        float* ranges[] = { h_ranges, s_ranges };
        int scale = 10;
        IplImage* hist_img = cvCreateImage( cvSize(h_bins*scale,s_bins*scale), 8, 3 );
        CvHistogram* hist;
        float max_value = 0;
        int h, s;

        cvCvtColor( src, hsv, CV_BGR2HSV );
        cvCvtPixToPlane( hsv, h_plane, s_plane, v_plane, 0 );
        hist = cvCreateHist( 2, hist_size, CV_HIST_ARRAY, ranges, 1 );
        cvCalcHist( planes, hist, 0, 0 );
        cvGetMinMaxHistValue( hist, 0, &max_value, 0, 0 );
        cvZero( hist_img );

        for( h = 0; h &lt; h_bins; h++ )
        {
            for( s = 0; s &lt; s_bins; s++ )
            {
                float bin_val = cvQueryHistValue_2D( hist, h, s );
                int intensity = cvRound(bin_val*255/max_value);
                cvRectangle( hist_img, cvPoint( h*scale, s*scale ),
                             cvPoint( (h+1)*scale - 1, (s+1)*scale - 1),
                             CV_RGB(intensity,intensity,intensity), /* graw a grayscale histogram.
                                                                       if you have idea how to do it
                                                                       nicer let us know */
                             CV_FILLED );
            }
        }

        cvNamedWindow( "Source", 1 );
        cvShowImage( "Source", src );

        cvNamedWindow( "H-S Histogram", 1 );
        cvShowImage( "H-S Histogram", hist_img );

        cvWaitKey(0);
    }
}
</pre>


<hr><h3><a name="decl_cvCalcBackProject">CalcBackProject</a></h3>
<p class="Blurb">Calculates back projection</p>
<pre>
void cvCalcBackProject( IplImage** img, CvArr* backProject, const CvHistogram* hist );
</pre><p><dl>
<dt>img<dd>Source images (though you may pass CvMat** as well).
<dt>backProject<dd>Destination back projection image of the same type as the source images.
<dt>hist<dd>Histogram.
</dl><p>
The function <a href="#decl_cvCalcBackProject">cvCalcBackProject</a> calculates the back project of the histogram. For
each tuple of pixels at the same position of all input single-channel
images the function puts the value of the histogram bin, corresponding to the tuple,
to the destination image. In terms of statistics, the value of each output image pixel
is probability of the observed tuple given the distribution (histogram).
For example, to find a red object in the picture, one may do the following:
<ol>
<li>Calculate a hue histogram for the red object assuming the image contains only
  this object. The histogram is likely to have a strong maximum, corresponding
  to red color.
<li>Calculate back projection of a hue plane of input image where the object is searched,
    using the histogram. Threshold the image.
<li>Find connected components in the resulting picture and choose the right
  component using some additional criteria, for example, the largest connected
  component.
</ol>
That is the approximate algorithm of Camshift color object tracker, except for the last step,
where CAMSHIFT algorithm is used to locate the object on the back projection given
the previous object position.


<hr><h3><a name="decl_cvCalcBackProjectPatch">CalcBackProjectPatch</a></h3>
<p class="Blurb">Locates a template within image by histogram comparison</p>
<pre>
void cvCalcBackProjectPatch( IplImage** img, CvArr* dst,
                             CvSize patchSize, CvHistogram* hist,
                             int method, float normFactor );
</pre><p><dl>
<dt>img<dd>Source images (though, you may pass CvMat** as well)
<dt>dst<dd>Destination image.
<dt>patchSize<dd>Size of patch slid though the source image.
<dt>hist<dd>Histogram
<dt>method<dd>Compasion method, passed to <a href="#decl_cvCompareHist">cvCompareHist</a> (see description of that function).
<dt>normFactor<dd>Normalization factor for histograms,
                  will affect normalization scale of destination image, pass 1. if unsure.
</dl><p>
The function <a href="#decl_cvCalcBackProjectPatch">cvCalcBackProjectPatch</a> calculates back projection by comparing
histograms of the source image patches with the given histogram. Taking
measurement results from some image at each location over ROI creates an array
<code>img</code>. These results might be one or more of hue, <code>x</code> derivative, <code>y</code> derivative,
Laplacian filter, oriented Gabor filter, etc. Each measurement output is
collected into its own separate image. The <code>img</code> image array is a collection of
these measurement images. A multi-dimensional histogram <code>hist</code> is constructed by
sampling from the <code>img</code> image array. The final histogram is normalized. The <code>hist</code>
histogram has as many dimensions as the number of elements in <code>img</code> array.
<p>
Each new image is measured and then converted into an <code>img</code> image array over a
chosen ROI. Histograms are taken from this <code>img</code> image in an area covered by a
"patch" with anchor at center as shown in the picture below.
The histogram is normalized using the parameter <code>norm_factor</code> so that it
may be compared with <code>hist</code>. The calculated histogram is compared to the model
histogram; <code>hist</code> uses the function <a href="#decl_cvCompareHist">cvCompareHist</a> with the comparison method=<code>method</code>).
The resulting output is placed at the location corresponding to the patch anchor in
the probability image <code>dst</code>. This process is repeated as the patch is slid over
the ROI. Iterative histogram update by subtracting trailing pixels covered by the patch and adding newly
covered pixels to the histogram can save a lot of operations, though it is not implemented yet.
</p>
<h4>Back Project Calculation by Patches</h4>
<p>
<img align="center" src="pics/backprojectpatch.png">
</p>


<hr><h3><a name="decl_cvCalcProbDensity">CalcProbDensity</a></h3>
<p class="Blurb">Divides one histogram by another</p>
<pre>
void  cvCalcProbDensity( const CvHistogram* hist1, const CvHistogram* hist2,
                         CvHistogram* histDens, double scale=255 );
</pre><p><dl>
<dt>hist1<dd>first histogram (divisor).
<dt>hist2<dd>second histogram.
<dt>histDens<dd>destination histogram.
</dl></p><p>
The function <a href="#decl_cvCalcProbDensity">cvCalcProbDensity</a> calculates the object probability density from
the two histograms as:</p>
<pre>
histDens(I)=0  if hist1(I)==0
            scale  if hist1(I)!=0 && hist2(I)&gt;hist1(I)
            hist2(I)*scale/hist1(I) if hist1(I)!=0 && hist2(I)&lt;=hist1(I)
</pre>
<p>
So the destination histogram bins are within [0,scale).
</p>


<hr><h3><a name="decl_cvCalcEMD2">CalcEMD2</a></h3>
<p class="Blurb">Computes "minimal work" distance between two weighted point configurations</p>
<pre>
float cvCalcEMD2( const CvArr* signature1, const CvArr* signature2, CvDisType distType,
                  float (*distFunc)(const float* f1, const float* f2, void* userParam ),
                  const CvArr* costMatrix, CvArr* flow,
                  float* lowerBound, void* userParam );
</pre><p><dl>
<dt>signature1<dd>First signature, <code>size1</code>&times;<code>dims+1</code> floating-point matrix.
    Each row stores the point weight followed by the point coordinates. The matrix is allowed to
    have a single column (weights only) if the user-defined cost matrix is used.
<dt>signature2<dd>Second signature of the same format as <code>signature1</code>, though the number
    of rows may be different. The total weights may be different, in this case an extra "dummy" point
    is added to either <code>signature1</code> or <code>signature2</code>.
<dt>distType<dd>Metrics used; <code>CV_DIST_L1, CV_DIST_L2</code>, and <code>CV_DIST_C</code> stand for one of
the standard metrics; <code>CV_DIST_USER</code> means that a user-defined function <code>distFunc</code> or
pre-calculated <code>costMatrix</code> is used.
<dt>distFunc<dd>The user-defined distance function.
                It takes coordinates of two points and returns the distance between the points.
<dt>costMatrix<dd>The user-defined <code>size1</code>&times;<code>size2</code> cost matrix.
                  At least one of <code>costMatrix</code> and <code>distFunc</code> must be NULL.
                  Also, if a cost matrix is used, lower boundary (see below) can not be calculated,
                  because it needs a metric function.
<dt>flow<dd>The resultant <code>size1</code>&times;<code>size2</code> flow matrix: <code>flow<sub>ij</sub></code> is a flow
            from i-th point of <code>signature1</code> to j-th point of <code>signature2</code>
<dt>lowerBound<dd>Optional output parameter: lower boundary of distance between the two signatures that
                  is a distance between mass centers. The lower boundary may not be calculated if
                  the user-defined cost matrix is used, the total weights of point configurations are
                  not equal, or there is the signatures consist of weights only
                  (i.e. the matrices have a single column).
<dt>userParam<dd>Pointer to optional data that is passed into the user-defined distance function.
</dl><p>
The function <a href="#decl_cvCalcEMD2">cvCalcEMD2</a> computes earth mover distance and/or a lower boundary of
the distance between the two weighted point configurations.
One of the application desctibed in <a href="#emd_paper">[RubnerSept98]</a> is multi-dimensional
histogram comparison for image retrieval.
EMD is a transportation problem that is solved using some modification of simplex algorithm,
thus the complexity is exponential in the worst case, though, it is much faster in average.
In case of real metric the lower boundary can be calculated even faster (using linear-time algorithm)
and it can be used to determine roughly whether the two
signatures are far enough so that they cannot relate to the same object.
</p>


<p><a name="emd_paper"></a><b>
[RubnerSept98] Y. Rubner. C. Tomasi, L.J. Guibas. The Earth Mover's Distance as a
Metric for Image Retrieval. Technical Report STAN-CS-TN-98-86,
Department of Computer Science, Stanford University, September
1998.
</b></p>


<hr><h2><a name="ch2_utilities">Utility Functions</a></h2>

<hr><h3><a name="decl_cvMatchTemplate">MatchTemplate</a></h3>
<p class="Blurb">Compares template against overlapped image regions</p>
<pre>
void cvMatchTemplate( const CvArr* I, const CvArr* T,
                      CvArr* result, int method );
</pre><p><dl>
<dt>I<dd>Image where the search is running.
           It should be single-chanel, 8-bit or 32-bit floating-point.
<dt>T<dd>Searched template; must be not greater than the source image and the same data type as the image.
<dt>R<dd>Image of comparison results; single-channel 32-bit floating-point. If <code>I</code> is
<code>W</code>&times;<code>H</code> and <code>T</code> is <code>w</code>&times;<code>h</code> then <code>R</code> must
be <code>W-w+1</code>&times;<code>H-h+1</code>.
<dt>method<dd>Specifies the way the template must be compared with image regions (see below).
</dl><p>
The function <a href="#decl_cvMatchTemplate">cvMatchTemplate</a> is similiar to <a href="#decl_cvCalcBackProjectPatch">cvCalcBackProjectPatch</a>.
It slids through <code>I</code>, compares <code>w</code>&times;<code>h</code>
patches against <code>T</code> using the specified method and stores the comparison results
to <code>result</code>. Here are the formular for the different comparison methods one may use
(the summation is done over template and/or the image patch: <code>x'=0..w-1, y'=0..h-1</code>):</p>
<pre>
method=CV_TM_SQDIFF:
R(x,y)=sum<sub>x',y'</sub>[T(x',y')-I(x+x',y+y')]<sup>2</sup>

method=CV_TM_SQDIFF_NORMED:
R(x,y)=sum<sub>x',y'</sub>[T(x',y')-I(x+x',y+y')]<sup>2</sup>/sqrt[sum<sub>x',y'</sub>T(x',y')<sup>2</sup>&bull;sum<sub>x',y'</sub>I(x+x',y+y')<sup>2</sup>]

method=CV_TM_CCORR:
R(x,y)=sum<sub>x',y'</sub>[T(x',y')&bull;I(x+x',y+y')]

method=CV_TM_CCORR_NORMED:
R(x,y)=sum<sub>x',y'</sub>[T(x',y')&bull;I(x+x',y+y')]/sqrt[sum<sub>x',y'</sub>T(x',y')<sup>2</sup>&bull;sum<sub>x',y'</sub>I(x+x',y+y')<sup>2</sup>]

method=CV_TM_CCOEFF:
R(x,y)=sum<sub>x',y'</sub>[T'(x',y')&bull;I'(x+x',y+y')],

where T'(x',y')=T(x',y') - 1/(w&bull;h)&bull;sum<sub>x",y"</sub>T(x",y") (mean template brightness=>0)
      I'(x+x',y+y')=I(x+x',y+y') - 1/(w&bull;h)&bull;sum<sub>x",y"</sub>I(x+x",y+y") (mean patch brightness=>0)

method=CV_TM_CCOEFF_NORMED:
R(x,y)=sum<sub>x',y'</sub>[T'(x',y')&bull;I'(x+x',y+y')]/sqrt[sum<sub>x',y'</sub>T'(x',y')<sup>2</sup>&bull;sum<sub>x',y'</sub>I'(x+x',y+y')<sup>2</sup>]

</pre>
After the function finishes comparison, the best matches can be found as global minimums (CV_TM_SQDIFF*)
or maximums (CV_TM_CCORR* and CV_TM_CCOEFF*) using <a href="OpenCVRef_BasicFuncs.htm#decl_cvMinMaxLoc">cvMinMaxLoc</a> function.
</p>

</body>
</html>

