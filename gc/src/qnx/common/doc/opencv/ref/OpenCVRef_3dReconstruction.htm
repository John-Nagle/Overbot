<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html><head>
<link rel="STYLESHEET" href="opencvref.css" charset="ISO-8859-1" type="text/css">
<title>3D Reconstruction Reference</title>
</head><body>

<h1>Camera Calibration and 3D Reconstruction Reference</h1>

<hr><p><ul>
<li><a href="#ch6_calibration">Camera Calibration Functions</a>
<ul>
<li><a href="#decl_cvCalibrateCamera">CalibrateCamera</a>
<li><a href="#decl_cvCalibrateCamera_64d">CalibrateCamera_64d</a>
<li><a href="#decl_cvRodrigues">Rodrigues</a>
<li><a href="#decl_cvUnDistortOnce">UnDistortOnce</a>
<li><a href="#decl_cvUnDistortInit">UnDistortInit</a>
<li><a href="#decl_cvUnDistort">UnDistort</a>
<li><a href="#decl_cvFindChessBoardCornerGuesses">FindChessBoardCornerGuesses</a>
</ul>
<li><a href="#ch6_pose">Pose Estimation</a>
<ul>
<li><a href="#decl_cvFindExtrinsicCameraParams">FindExtrinsicCameraParams</a>
<li><a href="#decl_cvFindExtrinsicCameraParams_64d">FindExtrinsicCameraParams_64d</a>
<li><a href="#decl_cvCreatePOSITObject">CreatePOSITObject</a>
<li><a href="#decl_cvPOSIT">POSIT</a>
<li><a href="#decl_cvReleasePOSITObject">ReleasePOSITObject</a>
<li><a href="#decl_cvCalcImageHomography">CalcImageHomography</a>
</ul>
<li><a href="#ch6_viewmorphing">View Morphing Functions</a>
<ul>
<li><a href="#decl_cvMakeScanlines">MakeScanlines</a>
<li><a href="#decl_cvPreWarpImage">PreWarpImage</a>
<li><a href="#decl_cvFindRuns">FindRuns</a>
<li><a href="#decl_cvDynamicCorrespondMulti">DynamicCorrespondMulti</a>
<li><a href="#decl_cvMakeAlphaScanlines">MakeAlphaScanlines</a>
<li><a href="#decl_cvMorphEpilinesMulti">MorphEpilinesMulti</a>
<li><a href="#decl_cvPostWarpImage">PostWarpImage</a>
<li><a href="#decl_cvDeleteMoire">DeleteMoire</a>
</ul>
<li><a href="#ch6_epilolar">Epipolar Geometry Functions</a>
<ul>
<li><a href="#decl_cvFindFundamentalMat">FindFundamentalMat</a>
<li><a href="#decl_cvComputeCorrespondEpilines">ComputeCorrespondEpilines</a>
</ul></ul></p>

<hr><h2><a name="ch6_calibration">Camera Calibration Functions</a></h2>

<hr><h3><a name="decl_cvCalibrateCamera">CalibrateCamera</a></h3>
<p class="Blurb">Calibrates camera with single precision</p>
<pre>
void cvCalibrateCamera( int numImages, int* numPoints, CvSize imageSize,
                        CvPoint2D32f* imagePoints32f, CvPoint3D32f* objectPoints32f,
                        CvVect32f distortion32f, CvMatr32f cameraMatrix32f,
                        CvVect32f transVects32f, CvMatr32f rotMatrs32f,
                        int useIntrinsicGuess );
</pre><p><dl>
<dt>numImages<dd>Number of the images.
<dt>numPoints<dd>Array of the number of points in each image.
<dt>imageSize<dd>Size of the image.
<dt>imagePoints32f<dd>Pointer to the images.
<dt>objectPoints32f<dd>Pointer to the pattern.
<dt>distortion32f<dd>Array of four distortion coefficients found.
<dt>cameraMatrix32f<dd>Camera matrix found.
<dt>transVects32f<dd>Array of translate vectors for each pattern position in the image.
<dt>rotMatrs32f<dd>Array of the rotation matrix for each pattern position in the image.
<dt>useIntrinsicGuess<dd>Intrinsic guess. If equal to 1, intrinsic guess is needed.
</dl></p><p>
The function <a href="#decl_cvCalibrateCamera">cvCalibrateCamera</a> calculates the camera parameters using information
points on the pattern object and pattern object images.

</p><hr><h3><a name="decl_cvCalibrateCamera_64d">CalibrateCamera_64d</a></h3>
<p class="Blurb">Calibrates camera with double precision</p>
<pre>
void cvCalibrateCamera_64d( int numImages, int* numPoints, CvSize imageSize,
                            CvPoint2D64d* imagePoints, CvPoint3D64d* objectPoints,
                            CvVect64d distortion, CvMatr64d cameraMatrix,
                            CvVect64d transVects, CvMatr64d rotMatrs,
                            int useIntrinsicGuess );
</pre><p><dl>
<dt>numImages<dd>Number of the images.
<dt>numPoints<dd>Array of the number of points in each image.
<dt>imageSize<dd>Size of the image.
<dt>imagePoints<dd>Pointer to the images.
<dt>objectPoints<dd>Pointer to the pattern.
<dt>distortion<dd>Distortion coefficients found.
<dt>cameraMatrix<dd>Camera matrix found.
<dt>transVects<dd>Array of the translate vectors for each pattern position on the
image.
<dt>rotMatrs<dd>Array of the rotation matrix for each pattern position on the image.
<dt>useIntrinsicGuess<dd>Intrinsic guess. If equal to 1, intrinsic guess is needed.
</dl></p><p>
The function <a href="#decl_cvCalibrateCamera_64d">cvCalibrateCamera_64d</a> is basically the same as the function
<a href="#decl_cvCalibrateCamera">cvCalibrateCamera</a>, but uses double precision.
</p>


<hr><h3><a name="decl_cvRodrigues">Rodrigues</a></h3>
<p class="Blurb">Converts rotation matrix to rotation vector and vice versa with single
precision</p>
<pre>
void  cvRodrigues( CvMat* rotMatrix, CvMat* rotVector,
                   CvMat* jacobian, int convType);
</pre><p><dl>
<dt>rotMatrix<dd>Rotation matrix (3x3),  32-bit or 64-bit floating point.
<dt>rotVector<dd>Rotation vector (3x1 or 1x3) of the same type as <em>rotMatrix</em>.
<dt>jacobian<dd>Jacobian matrix 3 &times; 9.
<dt>convType<dd>Type of conversion; must be <em>CV_RODRIGUES_M2V</em> for converting the matrix
to the vector or <em>CV_RODRIGUES_V2M</em> for converting the vector to the matrix.
</dl></p><p>
The function <a href="#decl_cvRodrigues">cvRodrigues</a> converts the rotation matrix to the rotation vector or
vice versa.
</p>


<hr><h3><a name="decl_cvUnDistortOnce">UnDistortOnce</a></h3>
<p class="Blurb">Corrects camera lens distortion</p>
<pre>
void cvUnDistortOnce( const CvArr* srcImage, CvArr* dstImage,
                      const float* intrMatrix,
                      const float* distCoeffs,
                      int interpolate=1 );
</pre><p><dl>
<dt>srcImage<dd>Source (distorted) image.
<dt>dstImage<dd>Destination (corrected) image.
<dt>intrMatrix<dd>Matrix of the camera intrinsic parameters (3x3).
<dt>distCoeffs<dd>Vector of the four distortion coefficients <em>k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub></em> and <em>p<sub>2</sub></em>.
<dt>interpolate<dd>Bilinear interpolation flag.
</dl></p><p>
The function <a href="#decl_cvUnDistortOnce">cvUnDistortOnce</a> corrects camera lens distortion in case of a single
image. Matrix of the camera intrinsic parameters and distortion coefficients <em>k<sub>1</sub>,
k<sub>2</sub> , p<sub>1</sub></em> , and <em>p<sub>2</sub></em> must be preliminarily calculated by the function
<a href="#decl_cvCalibrateCamera">cvCalibrateCamera</a>.
</p>


<hr><h3><a name="decl_cvUnDistortInit">UnDistortInit</a></h3>
<p class="Blurb">Calculates arrays of distorted points indices and interpolation coefficients</p>
<pre>
void cvUnDistortInit( const CvArr* srcImage, CvArr* undistMap,
                      const float* intrMatrix,
                      const float* distCoeffs,
                      int interpolate=1 );
</pre><p><dl>
<dt>srcImage<dd>Artibtrary source (distorted) image, the image size and number of channels do matter.
<dt>undistMap<dd>32-bit integer image of the same size as the source image (if <em>interpolate=0</em>) or
                 3 times wider than the source image (if <em>interpolate=1</em>).
<dt>intrMatrix<dd>Matrix of the camera intrinsic parameters.
<dt>distCoeffs<dd>Vector of the 4 distortion coefficients <em>k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub></em> and <em>p<sub>2</sub></em>.
<dt>interpolate<dd>Bilinear interpolation flag.
</dl></p><p>
The function <a href="#decl_cvUnDistortInit">cvUnDistortInit</a> calculates arrays of distorted points indices and
interpolation coefficients using known matrix of the camera intrinsic parameters
and distortion coefficients. It calculates undistortion map for <a href="#decl_cvUnDistort">cvUnDistort</a>.
<p>
Matrix of the camera intrinsic parameters and the distortion coefficients
may be calculated by <a href="#decl_cvCalibrateCamera">cvCalibrateCamera</a>.
</p>

<hr><h3><a name="decl_cvUnDistort">UnDistort</a></h3>
<p class="Blurb">Corrects camera lens distortion</p>
<pre>
void cvUnDistort( const void* srcImage, void* dstImage,
                  const void* undistMap, int interpolate=1 );
</pre><p><dl>
<dt>srcImage<dd>Source (distorted) image.
<dt>dstImage<dd>Destination (corrected) image.
<dt>undistMap<dd>Undistortion map, pre-calculated by <a href="#decl_cvUnDistortInit">cvUnDistortInit</a>.
<dt>interpolate<dd>Bilinear interpolation flag, the same as in <a href="#decl_cvUnDistortInit">cvUnDistortInit</a>.
</dl></p><p>
The function <a href="#decl_cvUnDistort">cvUnDistort</a> corrects camera lens distortion using previously
calculated undistortion map. It is faster than <a href="#decl_cvUnDistortOnce">cvUnDistortOnce</a>.
</p>


<hr><h3><a name="decl_cvFindChessBoardCornerGuesses">FindChessBoardCornerGuesses</a></h3>
<p class="Blurb">Finds approximate positions of internal corners of the chessboard</p>
<pre>
int cvFindChessBoardCornerGuesses( IplImage* img, IplImage* thresh, CvSize etalonSize,
                                   CvPoint2D32f* corners, int* cornerCount );
</pre><p><dl>
<dt>img<dd>Source chessboard view; must have the depth of <em>IPL_DEPTH_8U</em>.
<dt>thresh<dd>Temporary image of the same size and format as the source image.
<dt>etalonSize<dd>Number of inner corners per chessboard row and column. The width (the
number of columns) must be less or equal to the height (the number of rows).
<dt>corners<dd>Pointer to the corner array found.
<dt>cornerCount<dd>Signed value whose absolute value is the number of corners found. A
positive number means that a whole chessboard has been found and a negative
number means that not all the corners have been found.
</dl></p><p>
The function <a href="#decl_cvFindChessBoardCornerGuesses">cvFindChessBoardCornerGuesses</a> attempts to determine whether the input
image is a view of the chessboard pattern and locate internal chessboard
corners. The function returns non-zero value if all the corners have been found
and they have been placed in a certain order (row by row, left to right in every
row), otherwise, if the function fails to find all the corners or reorder them,
the function returns 0. For example, a simple chessboard has 8 x 8 squares and 7
x 7 internal corners, that is, points, where the squares are tangent. The word
"approximate" in the above description means that the corner coordinates found
may differ from the actual coordinates by a couple of pixels. To get more
precise coordinates, the user may use the function <a href="#decl_cvFindCornerSubPix">cvFindCornerSubPix</a>.
</p>


<hr><h2><a name="ch6_pose">Pose Estimation</a></h2>

<hr><h3><a name="decl_cvFindExtrinsicCameraParams">FindExtrinsicCameraParams</a></h3>
<p class="Blurb">Finds extrinsic camera parameters for pattern</p>
<pre>
void cvFindExtrinsicCameraParams( int numPoints, CvSize imageSize,
                                  CvPoint2D32f* imagePoints32f, CvPoint3D32f* objectPoints32f,
                                  CvVect32f focalLength32f, CvPoint2D32f principalPoint32f,
                                  CvVect32f distortion32f, CvVect32f rotVect32f,
                                  CvVect32f transVect32f );
</pre><p><dl>
<dt>numPoints<dd>Number of the points.
<dt>ImageSize<dd>Size of the image.
<dt>imagePoints32f<dd>Pointer to the image.
<dt>objectPoints32f<dd>Pointer to the pattern.
<dt>focalLength32f<dd>Focal length.
<dt>principalPoint32f<dd>Principal point.
<dt>distortion32f<dd>Distortion.
<dt>rotVect32f<dd>Rotation vector.
<dt>transVect32f<dd>Translate vector.
</dl></p><p>
The function <a href="#decl_cvFindExtrinsicCameraParams">cvFindExtrinsicCameraParams</a> finds the extrinsic parameters for the
pattern.
</p>


<hr><h3><a name="decl_cvFindExtrinsicCameraParams_64d">FindExtrinsicCameraParams_64d</a></h3>
<p class="Blurb">Finds extrinsic camera parameters for pattern with double precision</p>
<pre>
void cvFindExtrinsicCameraParams_64d( int numPoints, CvSize imageSize,
                                      CvPoint2D64d* imagePoints, CvPoint3D64d* objectPoints,
                                      CvVect64d focalLength, CvPoint2D64d principalPoint,
                                      CvVect64d distortion, CvVect64d rotVect,
                                      CvVect64d transVect );
</pre><p><dl>
<dt>numPoints<dd>Number of the points.
<dt>ImageSize<dd>Size of the image.
<dt>imagePoints<dd>Pointer to the image.
<dt>objectPoints<dd>Pointer to the pattern.
<dt>focalLength<dd>Focal length.
<dt>principalPoint<dd>Principal point.
<dt>distortion<dd>Distortion.
<dt>rotVect<dd>Rotation vector.
<dt>transVect<dd>Translate vector.
</dl></p><p>
The function <a href="#decl_cvFindExtrinsicCameraParams_64d">cvFindExtrinsicCameraParams_64d</a> finds the extrinsic parameters for
the pattern with double precision.
</p>

<hr><h3><a name="decl_cvCreatePOSITObject">CreatePOSITObject</a></h3>
<p class="Blurb">Initializes structure containing object information</p>
<pre>
CvPOSITObject* cvCreatePOSITObject( CvPoint3D32f* points, int numPoints );
</pre><p><dl>
<dt>points<dd>Pointer to the points of the 3D object model.
<dt>numPoints<dd>Number of object points.
</dl></p><p>
The function <a href="#decl_cvCreatePOSITObject">cvCreatePOSITObject</a> allocates memory for the object structure and
computes the object inverse matrix.
<p>
The preprocessed object data is stored in the structure <a href="#decl_CvPOSITObject">CvPOSITObject</a>, internal
for OpenCV, which means that the user cannot directly access the structure data.
The user may only create this structure and pass its pointer to the function.
</p>
<p>
Object is defined as a set of points given in a coordinate system. The function
<a href="#decl_cvPOSIT">cvPOSIT</a> computes a vector that begins at a camera-related coordinate system center
and ends at the <em>points[0]</em> of the object.
</p>
Once the work with a given object is finished, the function
<a href="#decl_cvReleasePOSITObject">cvReleasePOSITObject</a>
must be called to free memory.

</p><hr><h3><a name="decl_cvPOSIT">POSIT</a></h3>
<p class="Blurb">Implements POSIT algorithm</p>
<pre>
void cvPOSIT( CvPoint2D32f* imagePoints, CvPOSITObject* pObject,
              double focalLength, CvTermCriteria criteria,
              CvMatrix3* rotation, CvPoint3D32f* translation );
</pre><p><dl>
<dt>imagePoints<dd>Pointer to the object points projections on the 2D image plane.
<dt>pObject<dd>Pointer to the object structure.
<dt>focalLength<dd>Focal length of the camera used.
<dt>criteria<dd>Termination criteria of the iterative POSIT algorithm.
<dt>rotation<dd>Matrix of rotations.
<dt>translation<dd>Translation vector.
</dl></p><p>
The function <a href="#decl_cvPOSIT">cvPOSIT</a> implements POSIT algorithm. Image coordinates are given in a
camera-related coordinate system. The focal length may be retrieved using camera
calibration functions. At every iteration of the algorithm new perspective
projection of estimated pose is computed.
<p>
Difference norm between two projections is the maximal distance between
corresponding points. The parameter <em>criteria.epsilon</em> serves to stop the
algorithm if the difference is small.
</p>

</p><hr><h3><a name="decl_cvReleasePOSITObject">ReleasePOSITObject</a></h3>
<p class="Blurb">Deallocates 3D object structure</p>
<pre>
void cvReleasePOSITObject( CvPOSITObject** ppObject );
</pre><p><dl>
<dt>ppObject<dd>Address of the pointer to the object structure.
</dl></p><p>
The function <a href="#decl_cvReleasePOSITObject">cvReleasePOSITObject</a> releases memory previously allocated by the
function <a href="#decl_cvCreatePOSITObject">cvCreatePOSITObject</a>.
</p>

<hr><h3><a name="decl_cvCalcImageHomography">CalcImageHomography</a></h3>
<p class="Blurb">Calculates homography matrix for oblong planar object (e.g. arm)</p>
<pre>
void cvCalcImageHomography( float* line, CvPoint3D32f* center,
                            float* intrinsic, float homography[3][3]);
</pre><p><dl>
<dt>line<dd>the main object axis direction (vector (dx,dy,dz)).
<dt>center<dd>object center ((cx,cy,cz)).
<dt>intrinsic<dd>intrinsic camera parameters (3x3 matrix).
<dt>homography<dd>output homography matrix (3x3).
</dl></p><p>
The function <a href="#decl_cvCalcImageHomography">cvCalcImageHomography</a> calculates the homography matrix for the initial
image transformation from image plane to the plane, defined by 3D oblong object line (See
<u><font color=blue>Figure 6-10</font></u> in OpenCV Guide 3D Reconstruction Chapter).
</p>

<hr><h2><a name="ch6_viewmorphing">View Morphing Functions</a></h2>

<hr><h3><a name="decl_cvMakeScanlines">MakeScanlines</a></h3>
<p class="Blurb">Calculates scanlines coordinates for two cameras by fundamental matrix</p>
<pre>
void cvMakeScanlines( CvMatrix3* matrix, CvSize imgSize, int* scanlines1,
                      int* scanlines2, int* lens1, int* lens2, int* numlines );
</pre><p><dl>
<dt>matrix<dd>Fundamental matrix.
<dt>imgSize<dd>Size of the image.
<dt>scanlines1<dd>Pointer to the array of calculated scanlines of the first image.
<dt>scanlines2<dd>Pointer to the array of calculated scanlines of the second image.
<dt>lens1<dd>Pointer to the array of calculated lengths (in pixels) of the first image
scanlines.
<dt>lens2<dd>Pointer to the array of calculated lengths (in pixels) of the second image
scanlines.
<dt>numlines<dd>Pointer to the variable that stores the number of scanlines.
</dl></p><p>
The function <a href="#decl_cvMakeScanlines">cvMakeScanlines</a> finds coordinates of scanlines for two images.
<p>
This function returns the number of scanlines. The function does nothing except
calculating the number of scanlines if the pointers <em>scanlines1</em> or <em>scanlines2</em> are
equal to zero.
</p>

</p><hr><h3><a name="decl_cvPreWarpImage">PreWarpImage</a></h3>
<p class="Blurb">Rectifies image</p>
<pre>
void cvPreWarpImage( int numLines, IplImage* img, uchar* dst,
                     int* dstNums, int* scanlines );
</pre><p><dl>
<dt>numLines<dd>Number of scanlines for the image.
<dt>img<dd>Image to prewarp.
<dt>dst<dd>Data to store for the prewarp image.
<dt>dstNums<dd>Pointer to the array of lengths of scanlines.
<dt>scanlines<dd>Pointer to the array of coordinates of scanlines.
</dl></p><p>
The function <a href="#decl_cvPreWarpImage">cvPreWarpImage</a> rectifies the image so that the scanlines in the
rectified image are horizontal. The output buffer of size
<em>max(width,height)*numscanlines*3</em> must be allocated before calling the function.

</p><hr><h3><a name="decl_cvFindRuns">FindRuns</a></h3>
<p class="Blurb">Retrieves scanlines from rectified image and breaks them down into runs</p>
<pre>
void cvFindRuns( int numLines, uchar* prewarp_1, uchar* prewarp_2,
                 int* lineLens_1, int* lineLens_2,
                 int* runs_1, int* runs_2,
                 int* numRuns_1, int* numRuns_2 );
</pre><p><dl>
<dt>numLines<dd>Number of the scanlines.
<dt>prewarp_1<dd>Prewarp data of the first image.
<dt>prewarp_2<dd>Prewarp data of the second image.
<dt>lineLens_1<dd>Array of lengths of scanlines in the first image.
<dt>lineLens_2<dd>Array of lengths of scanlines in the second image.
<dt>runs_1<dd>Array of runs in each scanline in the first image.
<dt>runs_2<dd>Array of runs in each scanline in the second image.
<dt>numRuns_1<dd>Array of numbers of runs in each scanline in the first image.
<dt>numRuns_2<dd>Array of numbers of runs in each scanline in the second image.
</dl></p><p>
The function <a href="#decl_cvFindRuns">cvFindRuns</a> retrieves scanlines from the rectified image and breaks
each scanline down into several runs, that is, series of pixels of almost the
same brightness.

</p><hr><h3><a name="decl_cvDynamicCorrespondMulti">DynamicCorrespondMulti</a></h3>
<p class="Blurb">Finds correspondence between two sets of runs of two warped images</p>
<pre>
void cvDynamicCorrespondMulti( int lines, int* first, int* firstRuns,
                               int* second, int* secondRuns,
                               int* firstCorr, int* secondCorr );
</pre><p><dl>
<dt>lines<dd>Number of scanlines.
<dt>first<dd>Array of runs of the first image.
<dt>firstRuns<dd>Array of numbers of runs in each scanline of the first image.
<dt>second<dd>Array of runs of the second image.
<dt>secondRuns<dd>Array of numbers of runs in each scanline of the second image.
<dt>firstCorr<dd>Pointer to the array of correspondence information found for the first
runs.
<dt>secondCorr<dd>Pointer to the array of correspondence information found for the
second runs.
</dl></p><p>
The function <a href="#decl_cvDynamicCorrespondMulti">cvDynamicCorrespondMulti</a> finds correspondence between two sets of
runs of two images. Memory must be allocated before calling this function.
Memory size for one array of correspondence information is
<div><em>max( width,height )* numscanlines*3*sizeof ( int )</em> .

</p><hr><h3><a name="decl_cvMakeAlphaScanlines">MakeAlphaScanlines</a></h3>
<p class="Blurb">Calculates coordinates of scanlines of image from virtual camera</p>
<pre>
void cvMakeAlphaScanlines( int* scanlines_1, int* scanlines_2,
                           int* scanlinesA, int* lens,
                           int numlines, float alpha );
</pre><p><dl>
<dt>scanlines_1<dd>Pointer to the array of the first scanlines.
<dt>scanlines_2<dd>Pointer to the array of the second scanlines.
<dt>scanlinesA<dd>Pointer to the array of the scanlines found in the virtual image.
<dt>lens<dd>Pointer to the array of lengths of the scanlines found in the virtual
image.
<dt>numlines<dd>Number of scanlines.
<dt>alpha<dd>Position of virtual camera <em>(0.0 - 1.0)</em> .
</dl></p><p>
The function <a href="#decl_cvMakeAlphaScanlines">cvMakeAlphaScanlines</a> finds coordinates of scanlines for the virtual
camera with the given camera position.
<p>
Memory must be allocated before calling this function. Memory size for the array
of correspondence runs is <em>numscanlines*2*4*sizeof(int)</em> . Memory size for the
array of the scanline lengths is <em>numscanlines*2*4*sizeof(int)</em> .
</p>

</p><hr><h3><a name="decl_cvMorphEpilinesMulti">MorphEpilinesMulti</a></h3>
<p class="Blurb">Morphs two pre-warped images using information about stereo correspondence</p>
<pre>
void cvMorphEpilinesMulti( int lines, uchar* firstPix, int* firstNum,
                           uchar* secondPix, int* secondNum,
                           uchar* dstPix, int* dstNum,
                           float alpha, int* first, int* firstRuns,
                           int* second, int* secondRuns,
                           int* firstCorr, int* secondCorr );
</pre><p><dl>
<dt>lines<dd>Number of scanlines in the prewarp image.
<dt>firstPix<dd>Pointer to the first prewarp image.
<dt>firstNum<dd>Pointer to the array of numbers of points in each scanline in the first
image.
<dt>secondPix<dd>Pointer to the second prewarp image.
<dt>secondNum<dd>Pointer to the array of numbers of points in each scanline in the
second image.
<dt>dstPix<dd>Pointer to the resulting morphed warped image.
<dt>dstNum<dd>Pointer to the array of numbers of points in each line.
<dt>alpha<dd>Virtual camera position <em>(0.0 - 1.0)</em> .
<dt>first<dd>First sequence of runs.
<dt>firstRuns<dd>Pointer to the number of runs in each scanline in the first image.
<dt>second<dd>Second sequence of runs.
<dt>secondRuns<dd>Pointer to the number of runs in each scanline in the second image.
<dt>firstCorr<dd>Pointer to the array of correspondence information found for the first
runs.
<dt>secondCorr<dd>Pointer to the array of correspondence information found for the
second runs.
</dl></p><p>
The function <a href="#decl_cvMorphEpilinesMulti">cvMorphEpilinesMulti</a> morphs two pre-warped images using information
about correspondence between the scanlines of two images.

</p><hr><h3><a name="decl_cvPostWarpImage">PostWarpImage</a></h3>
<p class="Blurb">Warps rectified morphed image back</p>
<pre>
void cvPostWarpImage( int numLines, uchar* src, int* srcNums,
                      IplImage* img, int* scanlines );
</pre><p><dl>
<dt>numLines<dd>Number of the scanlines.
<dt>src<dd>Pointer to the prewarp image virtual image.
<dt>srcNums<dd>Number of the scanlines in the image.
<dt>img<dd>Resulting unwarp image.
<dt>scanlines<dd>Pointer to the array of scanlines data.
</dl></p><p>
The function <a href="#decl_cvPostWarpImage">cvPostWarpImage</a> warps the resultant image from the virtual camera by
storing its rows across the scanlines whose coordinates are calculated by
<a href="#decl_cvMakeAlphaScanlines">cvMakeAlphaScanlines</a>.

</p><hr><h3><a name="decl_cvDeleteMoire">DeleteMoire</a></h3>
<p class="Blurb">Deletes moire in given image</p>
<pre>
void cvDeleteMoire( IplImage* img );
</pre><p><dl>
<dt>img<dd>Image.
</dl></p><p>
The function <a href="#decl_cvDeleteMoire">cvDeleteMoire</a> deletes moire from the given image. The post-warped
image may have black (un-covered) points because of possible holes between
neighboring scanlines. The function deletes moire (black pixels) from the image
by substituting neighboring pixels for black pixels. If all the scanlines are
horizontal, the function may be omitted.</p>


<!-- by Valery Mosyagin -->

<hr><h2><a name="ch6_epipolar">Stereo Correspondence and Epipolar Geometry Functions</a></h2>

<hr><h3><a name="decl_cvFindFundamentalMat">FindFundamentalMat</a></h3>
<p class="Blurb">Calculates fundamental matrix from corresponding points in two images</p>
<pre>
int cvFindFundamentalMat( CvMat* points1,
                          CvMat* points2,
                          CvMat* fundMatr,
                          int    method,
                          double param1,
                          double param2,
                          CvMat* status=0);
</pre>
<p>
<dl>
<dt>points1  <dd>Array of the first image points of 2xN/Nx2 or 3xN/Nx3 size (N is number of points).
                 The point coordinates should be floating-point (single or double precision)
<dt>points2  <dd>Array of the second image points of the same size and format as <em>points1</em>
<dt>fundMatr <dd>The output fundamental matrix or matrices. Size 3x3 or 9x3 (7-point method can returns up to 3 matrices).

<dt>method <dd> Method for computing fundamental matrix
           <dd>CV_FM_7POINT - for 7-point algorithm. Number of points == 7
           <dd>CV_FM_8POINT - for 8-point algorithm. Number of points >= 8
           <dd>CV_FM_RANSAC - for RANSAC  algorithm. Number of points >= 8
           <dd>CV_FM_LMEDS  - for LMedS   algorithm. Number of points >= 8
<dt>param1 <dd>The parameter is used for RANSAC or LMedS methods only.
            It is the maximum distance from point to epipolar line,
            beyound which the point is considered bad and is not considered in
            further calculations. Usually it is set to 0.5 or 1.0.
<dt>param2 <dd>The parameter is used for RANSAC or LMedS methods only.
            It denotes the desirable level of confidense the matrix is the correct (up
            to some precision). It can be set to 0.99 for example.
<dt>status <dd>Array of N elements, every element of which is set to 1
            if the point was not rejected during the computation, 0 otherwise.
            The array is computed only in RANSAC and LMedS methods.
            For other methods it is set to all 1's.
            This is the optional parameter.</p>
</dl></p>

<p>
The epipolar geometry is described by the following equation:
<pre>p<sub>2</sub><sup>T</sup>*F*p<sub>1</sub>=0,</pre>
</p>

<p>where <em>F</em> is fundamental matrix, <em>p<sub>1</sub></em> and <em>p<sub>2</sub></em> are corresponding
points on the two images.
</p>

<p>
The function <em>FindFundamentalMat</em> calculates fundamental matrix using one of four
methods listed above and returns the number of fundamental matrix found: 0 if the
matrix could not be found, 1 or 3 if the matrix or matrices have been found successfully.
</p>

<p>
The calculated fundamental matrix may be passed further to <em>ComputeCorrespondEpilines</em>
function that computes coordinates of corresponding epilines on two images.</p>

<p>
For 7-point method uses exactly 7 points. It can find 1 or 3 fundamental
matrices. It returns number of the matrices found and if there is a room
in the destination array to keep all the detected matrices, stores all of them there,
otherwise it stores only one of the matrices.
</p>

<p>
All other methods use 8 or more points and return a single fundamental matrix.
</p>

<h4>Example. Fundamental matrix calculation</h4>
<pre>
int numPoints = 100;
CvMat* points1;
CvMat* points2;
CvMat* status;
CvMat* fundMatr;

points1  = cvCreateMat(2,numPoints,CV_32F);
points2  = cvCreateMat(2,numPoints,CV_32F);
status   = cvCreateMat(1,numPoints,CV_32F);

/* Fill the points here ... */

fundMatr = cvCreateMat(3,3,CV_32F);
int num = cvFindFundamentalMat(points1,points2,fundMatr,CV_FM_RANSAC,1.0,0.99,status);
if( num == 1 )
{
    printf("Fundamental matrix was found\n");
}
else
{
    printf("Fundamental matrix was not found\n");
}


/*====== Example of code for three matrixes ======*/
CvMat* points1;
CvMat* points2;
CvMat* fundMatr;

points1  = cvCreateMat(2,7,CV_32F);
points2  = cvCreateMat(2,7,CV_32F);

/* Fill the points here... */

fundMatr = cvCreateMat(9,3,CV_32F);
int num = cvFindFundamentalMat(points1,points2,fundMatr,CV_FM_7POINT,0,0,0);
printf("Found %d matrixes\n",num);
</pre>


<hr>
<h3><a name="decl_cvComputeCorrespondEpilines">ComputeCorrespondEpilines</a></h3>
<p class="Blurb">For every input point on one of image computes the
corresponding epiline on the other image</p>

<pre>
void cvComputeCorrespondEpilines( const CvMat* points,
                                  int pointImageID,
                                  CvMat* fundMatr,
                                  CvMat* corrLines);
</pre>
<p><dl>

<dt>points  <dd>The input points: 2xN or 3xN array (N number of points)
<dt>pointImageID <dd>Image ID there are points are located, 1 or 2
<dt>fundMatr <dd>Fundamental matrix
<dt>corrLines<dd>Computed epilines, 3xN array

</dl></p>

<p>
The function <em>ComputeCorrespondEpilines</em> computes the corresponding
epiline for every input point using the basic equation of epipolar line geometry:</p>



<p>
If points located on first image (ImageID=1), corresponding epipolar line
can be computed as:
<pre>l<sub>2</sub>=F*p<sub>1</sub></pre>

where <em>F</em> is fundamental matrix, <em>p<sub>1</sub></em> point on first image,
<em>l<sub>2</sub></em> corresponding epipolar line on second image.
</p>

If points located on second image (ImageID=2):
<pre>l<sub>1</sub>=F<sup>T</sup>*p<sub>2</sub></pre>



<p>
where <em>F</em> is fundamental matrix, <em>p<sub>2</sub></em> point on second image,
<em>l<sub>1</sub></em> corresponding epipolar line on first image
</p>

Each epipolar line is present by coefficients a,b,c of line equation:
<pre>a*x + b*y + c = 0</pre>

<p>
Also computed line normalized by <em>a<sup>2</sup>+b<sup>2</sup>=1</em>.
It's useful if distance from point to line must be computed later.
</p>

</body></html>



