<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html><head>
<link rel="STYLESHEET" href="opencvref.css" charset="ISO-8859-1" type="text/css">
<title>OpenCV: Experimental Functionality</title>
</head><body>

<h1>Experimental Functionality Reference</h1>

<p>
The functionality resides in cvaux library. To use it in your application, place
#include "cvaux.h" in your source files and:<ul>
<li>In case of Win32 link the app against cvaux.lib that is import library for cvaux.dll
<li>In case of Linux use -lcvaux compiler option
</ul></p>

<hr><p><ul>
<li><a href="#aux_facedetection">Object Detection Functions</a>
<ul>
<li><a href="#decl_CvHaar*">CvHaar*</a>
<li><a href="#decl_cvLoadHaarClassifierCascade">LoadHaarClassifierCascade</a>
<li><a href="#decl_cvReleaseHaarClassifierCascade">ReleaseHaarClassifierCascade</a>
<li><a href="#decl_cvCreateHidHaarClassifierCascade">CreateHidHaarClassifierCascade</a>
<li><a href="#decl_cvReleaseHidHaarClassifierCascade">ReleaseHidHaarClassifierCascade</a>
<li><a href="#decl_cvHaarDetectObjects">HaarDetectObjects</a>
<li><a href="#decl_cvSetImagesForHaarClassifierCascade">SetImagesForHaarClassifierCascade</a>
<li><a href="#decl_cvRunHaarClassifierCascade">RunHaarClassifierCascade</a>
<li><a href="#decl_cvGetHaarClassifierCascadeScale">GetHaarClassifierCascadeScale</a>
<li><a href="#decl_cvGetHaarClassifierCascadeWindowSize">GetHaarClassifierCascadeWindowSize</a>
</ul>
<li><a href="#aux_stereo">Stereo Correspondence Functions</a>
<ul>
<li><a href="#decl_cvFindStereoCorrespondence">FindStereoCorrespondence</a>
</ul>
<li><a href="#aux_3dTracking">3D Tracking Functions</a>
<ul>
<li><a href="#decl_cv3dTrackerCalibrateCameras">3dTrackerCalibrateCameras</a>
<li><a href="#decl_cv3dTrackerLocateObjects">3dTrackerLocateObjects</a>
</ul>
</ul></p>

<hr><h2><a name="aux_facedetection">Object Detection Functions</a></h2>

<p>
The object detector described below has been initially proposed by Paul Viola
<a href="#facedetect_paper1">[Viola01]</a> and improved by Rainer Lienhart
<a href="#facedetect_paper2">[Lienhart02]</a>.
First, a classifier (namely a <em>cascade of boosted classifiers working
with haar-like features</em>) is trained with a few hundreds of sample
views of a particular object (i.e., a face or a car), called positive
examples, that are scaled to the same size (say, 20x20), and negative examples
- arbitrary images of the same size.
</p><p>
After a classifier is trained, it can be applied to a region of interest (of
the same size as used during the training) in an input image. The
classifier outputs a "1" if the region is likely to show the object
(i.e., face/car), and "0" otherwise. To search for the object in the
whole image one can move the search window across the image and check
every location using the classifier. The classifier is designed so that it can
be easily "resized" in order to be able to find the objects of interest
at different sizes, which is more efficient than resizing the image itself. So,
to find an object of an unknown size in the image the scan procedure should be
done several times at different scales.
</p>
<p>
The word "cascade" in the classifier name means that the resultant classifier
consists of several simpler classifiers (<code>stages</code>) that are applied
subsequently to a region of interest until at some stage the candidate
is rejected or all the stages are passed. The word
"boosted" means that the classifiers at every stage of the cascade are complex
themselves and they are built out of basic classifiers using one of four
different <em>boosting</em> techniques (weighted voting). Currently
Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported.
The basic classifiers are decision-tree classifiers with at least
2 leaves. Haar-like features are the input to the basic classifers, and
are calculated as described below. The current algorithm uses the following
Haar-like features:</p>
<p>
<img src="pics/haarfeatures.png">
</p>
<p>
The feature used in a particular classifier is specified by its shape (1a,
2b etc.), position within the region of interest and the scale (this scale is
not the same as the scale used at the detection stage, though these two scales
are multiplied). For example, in case of the third line feature (2c) the
response is calculated as the difference between the sum of image pixels
under the rectangle covering the whole feature (including the two white
stripes and the black stripe in the middle) and the sum of the image
pixels under the black stripe multiplied by 3 in order to compensate for
the differences in the size of areas. The sums of pixel values over a
rectangular regions are calculated rapidly using integral images
(see below and <a href="OpenCVRef_ImageProcessing.htm#decl_cvIntegral">
cvIntegral</a> description).
</p><p>
To see the object detector at work, have a look at HaarFaceDetect demo.
</p><p>
The following reference is for the detection part only. There is a
separate application called <em>haartraining</em> that can train a
cascade of boosted classifiers from a set of samples.
See <em>opencv/apps/haartraining</em> for details.
</p>

<hr><h3><a name="decl_CvHaar*">CvHaarFeature, CvHaarClassifier, CvHaarStageClassifier, CvHaarClassifierCascade</a></h3>
<p class="Blurb">Boosted Haar classifier structures</p>
<pre>
#define CV_HAAR_FEATURE_MAX  3

/* a haar feature consists of 2-3 rectangles with appropriate weights */
typedef struct CvHaarFeature
{
    int  tilted; /* 0 means up-right feature, 1 means 45--rotated feature */

    /* 2-3 rectangles with weights of opposite signs and
       with absolute values inversely proportional to the areas of the rectangles.
       if rect[2].weight !=0, then
       the feature consists of 3 rectangles, otherwise it consists of 2 */
    struct
    {
        CvRect r;
        float weight;
    } rect[CV_HAAR_FEATURE_MAX];
} CvHaarFeature;

/* a single tree classifier (stump in the simplest case) that returns the response for the feature
   at the particular image location (i.e. pixel sum over subrectangles of the window) and gives out
   a value depending on the responce */
typedef struct CvHaarClassifier
{
    int count;
    /* number of nodes in the decision tree */
    CvHaarFeature* haarFeature;
    /* these are "parallel" arrays. Every index <code>i</code>
       corresponds to a node of the decision tree (root has 0-th index).

       left[i] - index of the left child (or negated index if the left child is a leaf)
       right[i] - index of the right child (or negated index if the right child is a leaf)
       threshold[i] - branch threshold. if feature responce is &lt;= threshold, left branch
                      is chosen, otherwise right branch is chosed.
       alpha[i] - output value correponding to the leaf. */
    float* threshold; /* array of decision thresholds */
    int* left; /* array of left-branch indices */
    int* right; /* array of right-branch indices */
    float* alpha; /* array of output values */
}
CvHaarClassifier;

/* a boosted battery of classifiers(=stage classifier):
   the stage classifier returns 1
   if the sum of the classifiers' responces
   is greater than <code>threshold</code> and 0 otherwise */
typedef struct CvHaarStageClassifier
{
    int  count; /* number of classifiers in the battery */
    float threshold; /* threshold for the boosted classifier */
    CvHaarClassifier* classifier; /* array of classifiers */
}
CvHaarStageClassifier;

/* cascade of stage classifiers */
typedef struct CvHaarClassifierCascade
{
    int  count; /* number of stages */
    CvSize origWindowSize; /* original object size (the cascade is trained for) */
    CvHaarStageClassifier* stageClassifier; /* array of stage classifiers */
}
CvHaarClassifierCascade;
</pre>
<p>
All the structures are used for representing a cascaded of boosted Haar
classifiers. The cascade has the following hierarchical structure:</p>
<pre>
    Cascade:
        Stage<sub>1</sub>:
            Classifier<sub>11</sub>:
                Feature<sub>11</sub>
            Classifier<sub>12</sub>:
                Feature<sub>12</sub>
            ...
        Stage<sub>2</sub>:
            Classifier<sub>21</sub>:
                Feature<sub>21</sub>
            ...
        ...
</pre><p>
The whole hierarchy can be constructed manually or loaded from a file or an
embedded base using function <a href="#decl_cvLoadHaarClassifierCascade">cvLoadHaarClassifierCascade</a>.
</p>


<hr><h3><a name="decl_cvLoadHaarClassifierCascade">cvLoadHaarClassifierCascade</a></h3>
<p class="Blurb">Loads a trained cascade classifier from file
                 or the classifier database embedded in OpenCV</p>
<pre>
CvHaarClassifierCascade*
cvLoadHaarClassifierCascade( const char* directory="&lt;default_face_cascade&gt;",
                             CvSize origWindowSize=cvSize(24,24));
</pre><p><dl>
<dt>directory<dd>Name of file containing the description of a trained cascade
                 classifier; or name in angle brackets of a cascade in the
                 classifier database embedded in OpenCV (only "&lt;default_face_cascade&gt;" is
                 supported now).
<dt>origWindowSize<dd>Original size of objects the cascade has been
                   trained on. Note that it is not stored in the cascade and therefore must
                   be specified separately.
</dl><p>
The function <a href="#decl_cvLoadHaarClassifierCascade">cvLoadHaarClassifierCascade</a>
loads a trained cascade of haar classifiers from a file or the classifier
database embedded in OpenCV. The base can be trained using <em>haartraining</em>
application (see opencv/apps/haartraining for details).
</p>

<hr><h3><a name="decl_cvReleaseHaarClassifierCascade">cvReleaseHaarClassifierCascade</a></h3>
<p class="Blurb">Releases haar classifier cascade</p>
<pre>
void cvReleaseHaarClassifierCascade( CvHaarClassifierCascade** cascade );
</pre>
<p><dl>
<dt>cascade<dd>Double pointer to the released cascade.
               The pointer is cleared by the function.
</dl>
<p>
The function <a href="#decl_cvReleaseHaarClassifierCascade">cvReleaseHaarClassifierCascade</a>
deallocates the cascade that has been created manually or by <a href="#decl_cvLoadHaarClassifierCascade">
cvLoadHaarClassifierCascade</a>.
</p>


<hr><h3><a name="decl_cvCreateHidHaarClassifierCascade">cvCreateHidHaarClassifierCascade</a></h3>
<p class="Blurb">Converts boosted classifier cascade to internal representation</p>
<pre>
/* hidden (optimized) representation of Haar classifier cascade */
typedef struct CvHidHaarClassifierCascade CvHidHaarClassifierCascade;

CvHidHaarClassifierCascade*
cvCreateHidHaarClassifierCascade( CvHaarClassifierCascade* cascade,
                                  const CvArr* sumImage=0,
                                  const CvArr* sqSumImage=0,
                                  const CvArr* tiltedSumImage=0,
                                  double scale=1 );
</pre><p><dl>
<dt>cascade<dd>original cascade that may be loaded from file using <a href="#decl_cvLoadHaarClassifierCascade">
               cvLoadHaarClassifierCascade</a>.
<dt>sumImage<dd>Integral (sum) single-channel image of 32-bit integer format.
                This image as well as the two subsequent images
                are used for fast feature evaluation and brightness/contrast normalization.
                They all can be retrieved from the input 8-bit single-channel image using
                function <a href="#decl_cvIntegral">cvIntegral</a>. Note that all the images
                are 1 pixel wider and 1 pixel taller than the source 8-bit image.
<dt>sqSumImage<dd>Square sum single-channel image of 64-bit floating-point format.
<dt>tiltedSumImage<dd>Tilted sum single-channel image of 32-bit integer format.
<dt>scale<dd>Initial scale (see <a href="#decl_cvSetImagesForHaarClassifierCascade">
             cvSetImagesForHaarClassifierCascade</a>).
</dl><p>
The function <a href="#decl_cvCreateHidHaarClassifierCascade">cvCreateHidHaarClassifierCascade</a>
converts pre-loaded cascade to internal faster representation. This step must
be done before the actual processing. The integral image pointers may be NULL,
in this case the images should be assigned later by <a href="#decl_cvSetImagesForHaarClassifierCascade">
cvSetImagesForHaarClassifierCascade</a>.
</p>


<hr><h3><a name="decl_cvReleaseHidHaarClassifierCascade">cvReleaseHidHaarClassifierCascade</a></h3>
<p class="Blurb">Releases hidden classifier cascade structure</p>
<pre>
void cvReleaseHidHaarClassifierCascade( CvHidHaarClassifierCascade** cascade );
</pre><p><dl>
<dt>cascade<dd>Double pointer to the released cascade. The pointer is cleared by the function.
</dl><p>
The function <a href="#decl_cvReleaseHidHaarClassifierCascade">cvReleaseHidHaarClassifierCascade</a>
deallocates structure that is an internal ("hidden") representation of haar
classifier cascade.</p>


<hr><h3><a name="decl_cvHaarDetectObjects">cvHaarDetectObjects</a></h3>
<p class="Blurb">Detects objects in the image</p>
<pre>
typedef struct CvAvgComp
{
    CvRect rect; /* bounding rectangle for the face (average rectangle of a group) */
    int neighbors; /* number of neighbor rectangles in the group */
}
CvAvgComp;

CvSeq* cvHaarDetectObjects( const IplImage* img, CvHidHaarClassifierCascade* cascade,
                            CvMemStorage* storage, double scale_factor=1.1,
                            int min_neighbors=3, int flags=0 );
</pre>
<p><dl>
<dt>img<dd>Image to detect objects in.
<dt>cascade<dd>Haar classifier cascade in internal representation.
<dt>storage<dd>Memory storage to store the resultant sequence of the
               object candidate rectangles.
<dt>scale_factor<dd>The factor by which the search window is scaled between the subsequent scans,
                    for example, 1.1 means increasing window by 10%.
<dt>min_neighbors<dd>Minimum number (minus 1) of neighbor rectangles
                     that makes up an object. All the groups of a smaller number of rectangles
                     than <code>min_neighbors</code>-1 are rejected.
                     If <code>min_neighbors</code> is 0, the function does not any
                     grouping at all and returns all the detected candidate rectangles,
                     which may be useful if the user wants to apply a customized grouping procedure.
<dt>flags<dd>Mode of operation. Currently the only flag that may be specified is <code>CV_HAAR_DO_CANNY_PRUNING</code>.
             If it is set, the function uses Canny edge detector to reject some image
             regions that contain too few or too much edges and thus can not contain the
             searched object. The particular threshold values are tuned for face detection
             and in this case the pruning speeds up the processing.
</dl>
<p>
The function <a href="#decl_cvHaarDetectObjects">cvHaarDetectObjects</a> finds
rectangular regions in the given image that are likely to contain objects
the cascade has been trained for and returns those regions as
a sequence of rectangles. The function scans the image several
times at different scales (see <a href="#decl_cvSetImagesForHaarClassifierCascade">
cvSetImagesForHaarClassifierCascade</a>). Each time it considers
overlapping regions in the image and applies the classifiers to the regions
using <a href="#decl_cvRunHaarClassifierCascade">cvRunHaarClassifierCascade</a>.
It may also apply some heuristics to reduce number of analyzed regions, such as
Canny prunning. After it has proceeded and collected the candidate rectangles
(regions that passed the classifier cascade), it groups them and returns a
sequence of average rectangles for each large enough group. The default
parameters (<code>scale_factor</code>=1.1, <code>min_neighbors</code>=3, <code>flags</code>=0)
are tuned for accurate yet slow face detection. For faster face detection on
real video images the better settings are (<code>scale_factor</code>=1.2, <code>min_neighbors</code>=2,
<code>flags</code>=CV_HAAR_DO_CANNY_PRUNING).
</p>
<h4>Example. Using cascade of Haar classifiers to find faces.</h4>
<pre>
#include "cv.h"
#include "cvaux.h"
#include "highgui.h"

CvHidHaarClassifierCascade* new_face_detector(void)
{
    CvHaarClassifierCascade* cascade = cvLoadHaarClassifierCascade("&lt;default_face_cascade&gt;", cvSize(24,24));
    /* images are assigned inside cvHaarDetectObject, so pass NULL pointers here */
    CvHidHaarClassifierCascade* hid_cascade = cvCreateHidHaarClassifierCascade( cascade, 0, 0, 0, 1 );
    /* the original cascade is not needed anymore */
    cvReleaseHaarClassifierCascade( &amp;cascade );
    return hid_cascade;
}

void detect_and_draw_faces( IplImage* image,
                            CvHidHaarClassifierCascade* cascade,
                            int do_pyramids )
{
    IplImage* small_image = image;
    CvMemStorage* storage = cvCreateMemStorage(0);
    CvSeq* faces;
    int i, scale = 1;

    /* if the flag is specified, down-scale the input image to get a
       performance boost w/o loosing quality (perhaps) */
    if( do_pyramids )
    {
        small_image = cvCreateImage( cvSize(image-&gt;width/2,image-&gt;height/2), IPL_DEPTH_8U, 3 );
        cvPyrDown( image, small_image, CV_GAUSSIAN_5x5 );
        scale = 2;
    }

    /* use the fastest variant */
    faces = cvHaarDetectObjects( small_image, cascade, storage, 1.2, 2, CV_HAAR_DO_CANNY_PRUNING );

    /* draw all the rectangles */
    for( i = 0; i &lt; faces-&gt;total; i++ )
    {
        /* extract the rectanlges only */
        CvRect face_rect = *(CvRect*)cvGetSeqElem( faces, i, 0 );
        cvRectangle( image, cvPoint(face_rect.x*scale,face_rect.y*scale),
                     cvPoint((face_rect.x+face_rect.width)*scale,
                             (face_rect.y+face_rect.height)*scale),
                     CV_RGB(255,0,0), 3 );
    }

    if( small_image != image )
        cvReleaseImage( &amp;small_image );
    cvReleaseMemStorage( &amp;storage );
}

/* takes image filename from the command line */
int main( int argc, char** argv )
{
    IplImage* image;
    if( argc==2 &amp;&amp; (image = cvLoadImage( argv[1], 1 )) != 0 )
    {
        CvHidHaarClassifierCascade* cascade = new_face_detector();
        detect_and_draw_faces( image, cascade, 1 );
        cvNamedWindow( "test", 0 );
        cvShowImage( "test", image );
        cvWaitKey(0);
        cvReleaseHidHaarClassifierCascade( &amp;cascade );
        cvReleaseImage( &amp;image );
    }

    return 0;
}
</pre>


<hr><h3><a name="decl_cvSetImagesForHaarClassifierCascade">cvSetImagesForHaarClassifierCascade</a></h3>
<p class="Blurb">Assigns images to the hidden cascade</p>
<pre>
void cvSetImagesForHaarClassifierCascade( CvHidHaarClassifierCascade* cascade,
                                          const CvArr* sumImage, const CvArr* sqSumImage,
                                          const CvArr* tiltedImage, double scale );
</pre>
<p><dl>
<dt>cascade<dd>Hidden Haar classifier cascade, created by <a href="#decl_cvCreateHidHaarClassifierCascade">
cvCreateHidHaarClassifierCascade</a>.
<dt>sumImage<dd>Integral (sum) single-channel image of 32-bit integer format. This image as well as the
                two subsequent images are used for fast feature evaluation and
                brightness/contrast normalization. They all can be retrieved from input 8-bit
                single-channel image using function <a href="#decl_cvIntegral">cvIntegral</a>.
                Note that all the images are 1 pixel wider and 1 pixel taller than the source
                8-bit image.
<dt>sqSumImage<dd>Square sum single-channel image of 64-bit floating-point format.
<dt>tiltedSumImage<dd>Tilted sum single-channel image of 32-bit integer format.
<dt>scale<dd>Window scale for the cascade. If <code>scale</code>=1, original window size is
             used (objects of that size are searched) - the same size as specified in
             <a href="#decl_cvLoadHaarClassifierCascade">cvLoadHaarClassifierCascade</a>
             (24x24 in case of "&lt;default_face_cascade&gt;"), if <code>scale</code>=2,
             a two times larger window is used (48x48 in case of default face cascade).
             While this will speed-up search about four times,
             faces smaller than 48x48 cannot be detected.
</dl>
<p>
The function <a href="#decl_cvSetImagesForHaarClassifierCascade">cvSetImagesForHaarClassifierCascade</a>
assigns images and/or window scale to the hidden classifier cascade.
If image pointers are NULL, the previously set images are used further
(i.e. NULLs mean "do not change images"). Scale parameter has no such a "protection" value, but
the previous value can be retrieved by <a href="#decl_cvGetHaarClassifierCascadeScale">
cvGetHaarClassifierCascadeScale</a> function and reused again. The function
is used to prepare cascade for detecting object of the particular size in the
particular image. The function is called internally by <a href="#decl_cvHaarDetectObjects">
cvHaarDetectObjects</a>, but it can be called by user if there is a need in
using lower-level function <a href="#decl_cvRunHaarClassifierCascade">cvRunHaarClassifierCascade</a>.
</p>


<hr>
<h3><a name="decl_cvRunHaarClassifierCascade">cvRunHaarClassifierCascade</a></h3>
<p class="Blurb">Runs cascade of boosted classifier at given image location</p>
<pre>
int cvRunHaarClassifierCascade( CvHidHaarClassifierCascade* cascade,
                                CvPoint pt, int startStage=0 );
</pre>
<p><dl>
<dt>cascade<dd>Hidden Haar classifier cascade.
<dt>pt<dd>Top-left corner of the analyzed
          region. Size of the region is a original window size scaled by the currenly set
          scale. The current window size may be retrieved using <a href="#decl_cvGetHaarClassifierCascadeWindowSize">
          cvGetHaarClassifierCascadeWindowSize</a> function.
<dt>startStage<dd>Initial zero-based index of the cascade stage to start from.
                  The function assumes that all the previous stages are passed.
                  This feature is used internally by <a href="#decl_cvHaarDetectObjects">
                  cvHaarDetectObjects</a> for better processor cache utilization.
</dl><p>
The function <a href="#decl_cvRunHaarHaarClassifierCascade">cvRunHaarHaarClassifierCascade</a>
runs Haar classifier cascade at a single image location. Before using this
function the integral images and the appropriate scale (=&gt; window size)
should be set using <a href="#decl_cvSetImagesForHaarClassifierCascade">cvSetImagesForHaarClassifierCascade</a>.
The function returns positive value if the analyzed rectangle passed all the classifier
stages (it is a candidate) and zero or negative value otherwise.
</p>


<hr><h3><a name="decl_cvGetHaarClassifierCascadeScale">cvGetHaarClassifierCascadeScale</a></h3>
<p class="Blurb">Retrieves the current scale of cascade of classifiers</p>
<pre>
double cvGetHaarClassifierCascadeScale( CvHidHaarClassifierCascadeScale* cascade );
</pre>
<p><dl>
<dt>cascade<dd>Hidden Haar classifier cascade.
</dl><p>
The function <a href="#decl_cvGetHaarHaarClassifierCascadeScale">cvGetHaarHaarClassifierCascadeScale</a>
retrieves the current scale factor for the search window of the Haar classifier
cascade. The scale can be changed by <a href="#decl_cvSetImagesForHaarClassifierCascade">
cvSetImagesForHaarClassifierCascade</a> by passing NULL image pointers and the new scale value.</p>


<hr><h3><a name="decl_cvGetHaarClassifierCascadeWindowSize">cvGetHaarClassifierCascadeWindowSize</a></h3>
<p class="Blurb">Retrieves the current search window size of cascade of classifiers</p>
<pre>
CvSize cvGetHaarClassifierCascadeWindowSize( CvHidHaarClassifierCascadeWindowSize* cascade );
</pre>
<p><dl>
<dt>cascade<dd>Hidden Haar classifier cascade.
</dl>
<p>
The function <a href="#decl_cvGetHaarHaarClassifierCascadeWindowSize">cvGetHaarHaarClassifierCascadeWindowSize</a>
retrieves the current search window size for the Haar classifier cascade. The
window size can be changed implicitly by setting appropriate scale.
</p>


<hr><h2><a name="aux_stereo">Stereo Correspondence Functions</a></h2>

<hr><h3><a name="decl_cvFindStereoCorrespondence">FindStereoCorrespondence</a></h3>
<p class="Blurb">Calculates disparity for stereo-pair</p>
<pre>
cvFindStereoCorrespondence(
                   const  CvArr* leftImage, const  CvArr* rightImage,
                   int     mode, CvArr*  depthImage,
                   int     maxDisparity,
                   double  param1, double  param2, double  param3,
                   double  param4, double  param5  );
</pre><p><dl>

<dt>leftImage<dd>Left image of stereo pair, rectified grayscale 8-bit image
<dt>rightImage<dd>Right image of stereo pair, rectified grayscale 8-bit image
<dt>mode<dd>Algorithm used to find a disparity (now only CV_DISPARITY_BIRCHFIELD is supported)
<dt>depthImage<dd>Destination depth image, grayscale 8-bit image that codes the scaled disparity,
                  so that the zero disparity (corresponding to the points that are very far from the cameras)
                  maps to 0, maximum disparity maps to 255.
<dt>maxDisparity<dd>Maximum possible disparity. The closer the objects to the cameras, the larger value should be specified here.
                    Too big values slow down the process significantly.
<dt>param1, param2, param3, param4, param5<dd> - parameters of algorithm.
For example, param1 is the constant occlusion penalty,
param2 is the constant match reward, param3 defines a highly reliable region
(set of contiguous pixels whose reliability is at least param3),
param4 defines a moderately reliable region, param5 defines a slightly reliable region.
If some parameter is omitted default value is used.
In Birchfield's algorithm param1 = 25,  param2 = 5, param3 = 12, param4 = 15, param5 = 25
(These values have been taken from
"Depth Discontinuities by Pixel-to-Pixel Stereo" Stanford University Technical Report STAN-CS-TR-96-1573, July 1996.)
</dl></p><p>

The function <a href="#decl_cvFindStereoCorrespondence">cvFindStereoCorrespondence</a> calculates disparity map
for two rectified grayscale images.

<p>Example. Calculating disparity for pair of 8-bit color images</h4>
<pre>
/*---------------------------------------------------------------------------------*/
IplImage* srcLeft = cvLoadImage("left.jpg",1);
IplImage* srcRight = cvLoadImage("right.jpg",1);
IplImage* leftImage = cvCreateImage(cvGetSize(srcLeft), IPL_DEPTH_8U, 1);
IplImage* rightImage = cvCreateImage(cvGetSize(srcRight), IPL_DEPTH_8U, 1);
IplImage* depthImage = cvCreateImage(cvGetSize(srcRight), IPL_DEPTH_8U, 1);

cvCvtColor(srcLeft, leftImage, CV_BGR2GRAY);
cvCvtColor(srcRight, rightImage, CV_BGR2GRAY);

cvFindStereoCorrespondence( leftImage, rightImage, CV_DISPARITY_BIRCHFIELD, depthImage, 50, 15, 3, 6, 8, 15 );
/*---------------------------------------------------------------------------------*/
</pre>
<p>And here is the example stereo pair that can be used to test the example</p>
<p>
<img src="pics/left.jpg">
<img src="pics/right.jpg">
</p>


<hr><h2><a name="aux_3dTracking">3D Tracking Functions</a></h2>

<p>The section discusses functions for tracking objects in 3d space using a stereo camera.
Besides C API, there is DirectShow <a href="../appPage/3dTracker/3dTrackerFilter.htm">3dTracker</a> filter
and the wrapper application <a href="../appPage/3dTracker/3dTracker.htm">3dTracker</a>.
<a href="../appPage/3dTracker/3dTrackerTesting.htm">Here</a> you may find a description how to test the filter on sample data.</p>

<hr><h3><a name="decl_cv3dTrackerCalibrateCameras">3dTrackerCalibrateCameras</a></h3>
<p class="Blurb">Simultaneously determines position and orientation of multiple cameras</p>
<pre>
CvBool cv3dTrackerCalibrateCameras(int num_cameras,
           const Cv3dTrackerCameraIntrinsics camera_intrinsics[],
           CvSize checkerboard_size,
           IplImage *samples[],
           Cv3dTrackerCameraInfo camera_info[]);
</pre><p><dl>
<dt>num_cameras<dd>the number of cameras to calibrate. This is the size of each of the
three array parameters.
<dt>camera_intrinsics<dd>camera intrinsics for each camera, such as determined by CalibFilter.
<dt>checkerboard_size<dd>the width and height (in number of squares) of the checkerboard.
<dt>samples<dd>images from each camera, with a view of the checkerboard.
<dt>camera_info<dd>filled in with the results of the camera calibration. This is passed into
<a href="#decl_cv3dTrackerLocateObjects">3dTrackerLocateObjects</a> to do tracking.
</dl></p>
<p>
The function <a href="#decl_cv3dTrackerCalibrateCameras">cv3dTrackerCalibrateCameras</a>
searches for a checkerboard of the specified size in each
of the images. For each image in which it finds the checkerboard, it fills
in the corresponding slot in <code>camera_info</code> with the position
and orientation of the camera
relative to the checkerboard and sets the <code>valid</code> flag.
If it finds the checkerboard in all the images, it returns true;
otherwise it returns false.
</p><p>
This function does not change the members of the <code>camera_info</code> array
that correspond to images in which the checkerboard was not found.
This allows you to calibrate each camera independently, instead of
simultaneously.
To accomplish this, do the following:
<ol>
<li>clear all the <code>valid</code> flags before calling this function the first time;</LI>
<li>call this function with each set of images;</LI>
<li> check all the <code>valid</code> flags after each call.
When all the <code>valid</code> flags are set, calibration is complete.
</li>
</ol>
Note that this method works well only if the checkerboard is rigidly mounted;
if it is handheld, all the cameras should be calibrated simultanously
to get an accurate result.
To ensure that all cameras are calibrated simultaneously,
ignore the <code>valid</code> flags and
use the return value to decide when calibration is complete.
</p>

<hr><h3><a name="decl_cv3dTrackerLocateObjects">3dTrackerLocateObjects</a></h3>
<p class="Blurb">Determines 3d location of tracked objects</p>
<pre>
int  cv3dTrackerLocateObjects(int num_cameras,
         int num_objects,
         const Cv3dTrackerCameraInfo camera_info[],
         const Cv3dTracker2dTrackedObject tracking_info[],
         Cv3dTrackerTrackedObject tracked_objects[]);
</pre><p><dl>
<dt>num_cameras<dd>the number of cameras.
<dt>num_objects<dd>the maximum number of objects found by any camera. (Also the
maximum number of objects returned in <code>tracked_objects</code>.)
<dt>camera_info<dd>camera position and location information for each camera,
as determined by <a href="#decl_cv3dTrackerCalibrateCameras">3dTrackerCalibrateCameras</a>.
<dt>tracking_info<dd>the 2d position of each object as seen by each camera. Although
this is specified as a one-dimensional array, it is actually a
two-dimensional array:
<code>const Cv3dTracker2dTrackedObject tracking_info[num_cameras][num_objects]</code>.
The <code>id</code> field of any unused slots must be -1. Ids need not
be ordered or consecutive.
<dt>tracked_objects<dd>filled in with the results.
</dl></p>
<p>
The function <a href="#decl_cv3dTrackerLocateObjects">cv3dTrackerLocateObjects</a>
determines the 3d position of tracked objects
based on the 2d tracking information from multiple cameras and
the camera position and orientation information computed by
<a href="#decl_3dTrackerCalibrateCameras">3dTrackerCalibrateCameras</a>.
It locates any objects with the same <code>id</code> that are tracked by more
than one camera.
It fills in the <code>tracked_objects</code> array and
returns the number of objects located. The <code>id</code> fields of
any unused slots in <code>tracked_objects</code> are set to -1.
</p>

</body>
</html>

